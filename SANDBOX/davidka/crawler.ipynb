{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9226cb-bb4b-4fa1-aaa6-f314ec42036e",
   "metadata": {},
   "source": [
    "```python\n",
    "## \\file /sandbox/davidka/crawler.py\n",
    "# -*- coding: utf-8 -*-\n",
    "#! .pyenv/bin/python3\n",
    "```\n",
    "\n",
    "Модуль для сбора данных со страниц различных сайтов\n",
    "=====================================================\n",
    "\n",
    "\n",
    "```rst\n",
    ".. module:: sandbox.davidka.crawler\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91f30c-065e-4f93-9243-f6eda09f88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7f074-3c87-40f9-ab5e-ed89515e49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb # <- трасировка и точки останова\n",
    "import asyncio, random\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "import header\n",
    "from header import __root__\n",
    "from src import gs\n",
    "from src.webdriver.ai_browser import Driver\n",
    "from src.utils.jjson import j_loads, j_loads_ns, j_dumps\n",
    "from src.utils.file import read_text_file, save_text_file, get_filenames_from_directory \n",
    "from src.utils.printer import pprint as print\n",
    "from src.logger.logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941e20f-8bc2-4b16-b112-1c2e00b25731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(SimpleNamespace):\n",
    "    ENDPOINT:Path = __root__/'SANDBOX'/'davidka'\n",
    "    mining_data_path:Path = ENDPOINT/'mining_data'\n",
    "    crawl_files_list:list = get_filenames_from_directory(mining_data_path, 'json')\n",
    "    task_description =  Path(ENDPOINT/ 'tasks'/ 'grab_product_page.md').read_text(encoding='utf-8')\n",
    "\n",
    "driver:Driver = Driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee7b84-d828-4127-8496-b8adaaf36a0b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_products_urls_list_from_files(crawl_files_list:list = []) -> list:\n",
    "    \"\"\"\n",
    "   Функция читает содержимое файлов  в директории `mining_data`, перемешивает их и возвращает одним большим списком\n",
    "   \"\"\"\n",
    "    products_urls_list = []\n",
    "    for filename in crawl_files_list or Config.crawl_files_list:\n",
    "        try:\n",
    "            file_path = Config.mining_data_path / filename\n",
    "            crawl_data = j_loads(file_path)['products']\n",
    "            for product in crawl_data:\n",
    "                products_urls_list.append(product['product_url'])\n",
    "        except Exception as ex:\n",
    "            logger.error(f'Ошибка при обработке файла/n {filename=}/n', ex)\n",
    "            ...\n",
    "    random.shuffle(products_urls_list)\n",
    "    return products_urls_list if isinstance(products_urls_list, list) else [products_urls_list]\n",
    "\n",
    "def yield_product_urls_from_files(directory: Path = Config.mining_data_path, pattern: str = 'json'):\n",
    "    \"\"\"\n",
    "    Функция возвращает генератор списка `url` Применяется на больших объемах данных\n",
    "    \"\"\"\n",
    "    filenames = get_filenames_from_directory(directory, pattern)\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            file_path = directory / filename\n",
    "            crawl_data = j_loads(file_path)['products']\n",
    "            for product in crawl_data:\n",
    "                yield product['product_url']\n",
    "        except Exception as ex:\n",
    "            logger.error(f'Ошибка при обработке файла {filename=}', ex)\n",
    "            ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4270213-b012-4cf6-9595-b2c319b5a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Через генератор для совсем больших данных\n",
    "# for product_url in yield_product_urls_from_files():\n",
    "\n",
    "for product_url in get_products_urls_list_from_files():\n",
    "    try:\n",
    "        logger.info(f'Обработка URL: {product_url}')\n",
    "        task = Config.task_description.replace('<URL>', product_url)\n",
    "        \n",
    "        extracted_data = await driver.run_task(task)\n",
    "        ipdb.set_trace()\n",
    "        print(extracted_data)\n",
    "        ...\n",
    "    except Exception as ex:\n",
    "        logger.error(f'Ошибка при обработке {product_url=}', ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff0892-7c0d-400f-a4a2-8e68f1d9ec5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
