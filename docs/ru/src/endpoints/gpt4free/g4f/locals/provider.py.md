# Модуль provider.py

## Обзор

Модуль `provider.py` предназначен для обеспечения локального взаимодействия с моделями GPT4All. Он содержит функции для поиска моделей, загрузки (при необходимости) и создания завершений (completions) на основе предоставленных сообщений. Модуль абстрагирует процесс работы с локальными моделями GPT4All, упрощая интеграцию в проект `hypotez`.

## Подробнее

Этот модуль служит адаптером для локальных моделей GPT4All, позволяя использовать их в качестве поставщиков (providers) в системе `hypotez`. Он включает в себя функции для поиска моделей в различных директориях, загрузки моделей, если они не найдены локально, и создания текстовых завершений на основе предоставленных сообщений.

## Функции

### `find_model_dir`

```python
def find_model_dir(model_file: str) -> str:
    """Функция выполняет поиск директории, содержащей указанный файл модели.

    Функция ищет файл модели в нескольких местах: в локальной директории модуля,
    в директории проекта и в текущей рабочей директории.

    Args:
        model_file (str): Имя файла модели, который нужно найти.

    Returns:
        str: Путь к директории, содержащей файл модели.
             Если файл не найден, возвращает путь к новой директории для моделей.

    """
```

****

1.  Определяет абсолютный путь к локальной директории, где находится текущий скрипт.
2.  Определяет путь к директории проекта, находящейся на уровень выше локальной.
3.  Формирует путь к файлу модели в новой директории моделей (в директории проекта).
4.  Проверяет, существует ли файл модели в новой директории. Если да, возвращает путь к этой директории.
5.  Формирует путь к файлу модели в старой директории моделей (в локальной директории).
6.  Проверяет, существует ли файл модели в старой директории. Если да, возвращает путь к этой директории.
7.  Выполняет поиск файла модели во всех поддиректориях текущей рабочей директории.
8.  Если файл найден в какой-либо из поддиректорий, возвращает путь к этой директории.
9.  Если файл не найден ни в одной из проверенных директорий, возвращает путь к новой директории моделей (в директории проекта).

**Примеры:**

Предположим, что `model_file` это `"ggml-model.bin"`.

```python
# Если файл найден в новой директории моделей
find_model_dir("ggml-model.bin")  # Возвращает "/путь/к/проекту/models"

# Если файл найден в старой директории моделей
find_model_dir("ggml-model.bin")  # Возвращает "/путь/к/локальной/директории/models"

# Если файл найден в текущей рабочей директории
find_model_dir("ggml-model.bin")  # Возвращает "./"

# Если файл не найден нигде
find_model_dir("ggml-model.bin")  # Возвращает "/путь/к/проекту/models"
```

## Классы

### `LocalProvider`

**Описание**:

Класс `LocalProvider` предоставляет статический метод для создания завершений с использованием локальных моделей GPT4All.

**Методы**:

*   `create_completion`

### `create_completion`

```python
    @staticmethod
    def create_completion(model: str, messages: Messages, stream: bool = False, **kwargs):
        """Создает текстовое завершение с использованием локальной модели GPT4All.

        Функция загружает указанную модель, формирует запрос на основе предоставленных сообщений
        и генерирует завершение. Поддерживает потоковую генерацию.

        Args:
            model (str): Имя модели для использования.
            messages (Messages): Список сообщений для формирования запроса.
            stream (bool, optional): Если `True`, включает потоковую генерацию. По умолчанию `False`.
            **kwargs: Дополнительные аргументы.

        Returns:
            Generator[str, None, None] | str: Генератор токенов, если `stream` is `True`,
                                               или строка с завершением, если `stream` is `False`.

        Raises:
            ValueError: Если модель не найдена или не реализована.
            ValueError: Если файл модели не найден.

        """
```

****

1.  Инициализирует глобальную переменную `MODEL_LIST`, если она еще не инициализирована, с помощью функции `get_models()`.
2.  Проверяет, есть ли указанная модель в списке доступных моделей (`MODEL_LIST`). Если нет, вызывает исключение `ValueError`.
3.  Извлекает информацию о модели из `MODEL_LIST`.
4.  Определяет директорию, в которой находится файл модели, с помощью функции `find_model_dir()`.
5.  Проверяет, существует ли файл модели в найденной директории. Если нет, предлагает пользователю загрузить модель и, в случае положительного ответа, выполняет загрузку с помощью `GPT4All.download_model()`. Если пользователь отказывается или возникает другая ошибка, вызывает исключение `ValueError`.
6.  Инициализирует объект `GPT4All` с указанием имени файла модели, пути к модели и других параметров.
7.  Формирует системное сообщение из сообщений с ролью "system".
8.  Формирует строку разговора из сообщений с ролями, отличными от "system".
9.  Определяет функцию `should_not_stop()`, которая определяет, нужно ли остановить генерацию на основе текущего токена.
10. Запускает сессию чата с моделью, используя системное сообщение и шаблон запроса.
11. В зависимости от значения параметра `stream`, генерирует завершение потоковым или непотоковым способом.
12. Если `stream` is `True`, генерирует токены и возвращает их в виде генератора.
13. Если `stream` is `False`, генерирует завершение целиком и возвращает его в виде строки.

**Внутренние функции:**

*   `should_not_stop(token_id: int, token: str) -> bool`

    ```python
    def should_not_stop(token_id: int, token: str):
        """Определяет, следует ли остановить генерацию на основе текущего токена.

        Args:
            token_id (int): ID текущего токена.
            token (str): Текущий токен.

        Returns:
            bool: `True`, если генерацию не следует останавливать, `False` в противном случае.

        """
    ```

    ****

    Функция проверяет, содержит ли текущий токен строку "USER". Если да, возвращает `False`, указывая на необходимость остановить генерацию. В противном случае возвращает `True`.

**Примеры:**

```python
# Непотоковая генерация
completion = LocalProvider.create_completion(model="ggml-model.bin", messages=[{"role": "user", "content": "Hello"}])
print(completion)

# Потоковая генерация
for token in LocalProvider.create_completion(model="ggml-model.bin", messages=[{"role": "user", "content": "Hello"}], stream=True):
    print(token, end="")
```