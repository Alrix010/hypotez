# Модуль `provider.py`

## Обзор

Модуль `provider.py` предоставляет класс `LocalProvider`, который позволяет использовать локальные модели GPT4All для создания завершений текста. Он включает функции для поиска файлов моделей, загрузки моделей и взаимодействия с ними.

## Подробнее

Этот модуль является частью проекта `hypotez` и предназначен для обеспечения возможности использования локальных моделей обработки естественного языка без необходимости подключения к внешним API. Он определяет, где хранятся файлы моделей, загружает их и использует для генерации текста на основе предоставленных сообщений.

## Функции

### `find_model_dir`

```python
def find_model_dir(model_file: str) -> str:
    """
    Находит директорию, в которой находится файл модели.

    Args:
        model_file (str): Имя файла модели.

    Returns:
        str: Путь к директории, содержащей файл модели.

    Как работает функция:
    1. Определяет текущую директорию модуля и корневую директорию проекта.
    2. Проверяет, находится ли файл модели в директории `models` внутри корневой директории проекта.
    3. Если файл не найден, проверяет, находится ли он в директории `models` внутри текущей директории модуля.
    4. Если файл по-прежнему не найден, выполняет поиск файла модели во всех поддиректориях текущей рабочей директории.
    5. Если файл не найден ни в одной из этих директорий, возвращает путь к директории `models` внутри корневой директории проекта.

    ASCII flowchart:

    A [Определение директорий]
    ↓
    B [Проверка в project_dir/models]
    ↓
    C [Проверка в local_dir/models]
    ↓
    D [Поиск в поддиректориях]
    ↓
    E [Возврат project_dir/models]

    Примеры:
    >>> find_model_dir("ggml-model.bin")
    '/путь/к/проекту/models'
    """
    ...
```

### Классы

### `LocalProvider`
**Описание**: Класс, предоставляющий методы для создания завершений текста с использованием локальных моделей GPT4All.

**Принцип работы**: Класс использует статический метод `create_completion` для загрузки модели, если она еще не загружена, и генерации текста на основе предоставленных сообщений. Он также обрабатывает случай, когда файл модели не найден, и предлагает пользователю загрузить его.

```python
class LocalProvider:
    """
    Предоставляет методы для создания завершений текста с использованием локальных моделей GPT4All.
    """
```
#### `LocalProvider.create_completion`

```python
    @staticmethod
    def create_completion(model: str, messages: Messages, stream: bool = False, **kwargs):
        """
        Создает завершение текста с использованием локальной модели GPT4All.

        Args:
            model (str): Имя модели.
            messages (Messages): Список сообщений для передачи модели.
            stream (bool, optional): Если `True`, возвращает генератор токенов. По умолчанию `False`.
            **kwargs: Дополнительные аргументы для передачи модели.

        Returns:
            Generator[str, None, None] | str: Генератор токенов или завершенный текст.

        Raises:
            ValueError: Если модель не найдена или не реализована.
            ValueError: Если файл модели не найден.

        Как работает функция:
        1. Проверяет, инициализирован ли список моделей (`MODEL_LIST`). Если нет, инициализирует его с помощью `get_models()`.
        2. Проверяет, существует ли указанная модель в списке моделей. Если нет, вызывает исключение `ValueError`.
        3. Получает информацию о модели из списка моделей.
        4. Находит директорию, в которой находится файл модели, с помощью функции `find_model_dir()`.
        5. Проверяет, существует ли файл модели в найденной директории. Если нет, предлагает пользователю загрузить его.
        6. Инициализирует модель GPT4All с указанными параметрами.
        7. Формирует системное сообщение и историю разговора из списка сообщений.
        8. Определяет функцию `should_not_stop()`, которая определяет, следует ли остановить генерацию текста.
        9. Создает сессию чата с моделью и генерирует текст на основе истории разговора.
        10. Если `stream` равен `True`, возвращает генератор токенов. В противном случае возвращает завершенный текст.

        ASCII flowchart:

        A [Проверка и инициализация MODEL_LIST]
        ↓
        B [Проверка существования модели]
        ↓
        C [Поиск директории модели]
        ↓
        D [Проверка наличия файла модели]
        ↓
        E [Инициализация GPT4All]
        ↓
        F [Формирование сообщений]
        ↓
        G [Генерация текста]
        ↓
        H [Возврат результата (генератор или текст)]

        Примеры:
        >>> LocalProvider.create_completion(model="model_name", messages=[{"role": "user", "content": "Hello"}], stream=False)
        "Hello, how can I help you?"
        """
        ...
```
```python
def should_not_stop(token_id: int, token: str):
    """
    Определяет, следует ли остановить генерацию текста на основе текущего токена.

    Args:
        token_id (int): Идентификатор токена.
        token (str): Текущий токен.

    Returns:
        bool: `True`, если генерацию текста следует продолжить, `False` в противном случае.
    """
    return "USER" not in token
```
## Примеры

### Использование `find_model_dir`
```python
model_dir = find_model_dir("ggml-model.bin")
print(f"Директория модели: {model_dir}")
```

### Использование `LocalProvider.create_completion`
```python
messages = [{"role": "user", "content": "Hello"}]
completion = LocalProvider.create_completion(model="model_name", messages=messages, stream=False)
print(f"Завершение: {completion}")