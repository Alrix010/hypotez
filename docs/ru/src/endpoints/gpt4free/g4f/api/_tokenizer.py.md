# Модуль для токенизации текста

## Обзор

Модуль предоставляет функциональность для токенизации текста с использованием библиотеки `tiktoken`.
Он позволяет определить количество токенов в тексте и получить список закодированных токенов.

## Подробней

Модуль предназначен для работы с большими языковыми моделями, такими как GPT-3.5-turbo, и предоставляет инструменты для анализа текста на основе токенов. Токенизация важна для понимания структуры текста и его обработки моделями машинного обучения.

## Функции

### `tokenize`

```python
def tokenize(text: str, model: str = 'gpt-3.5-turbo') -> Union[int, str]:
    """ Функция выполняет токенизацию текста с использованием указанной модели.
    Args:
        text (str): Текст для токенизации.
        model (str, optional): Название модели для токенизации. По умолчанию 'gpt-3.5-turbo'.

    Returns:
        Union[int, str]: Количество токенов в тексте и список закодированных токенов.

    Raises:
        Exception: Если возникает ошибка при токенизации текста.

    Example:
        >>> tokenize('Пример текста для токенизации', model='gpt-3.5-turbo')
        (5, [1234, 5678, 9012, 3456, 7890])
    """
    ...
```

**Назначение**: Токенизация текста и определение количества токенов.

**Параметры**:
- `text` (str): Текст, который нужно токенизировать.
- `model` (str, optional): Модель, используемая для токенизации (например, 'gpt-3.5-turbo'). По умолчанию 'gpt-3.5-turbo'.

**Возвращает**:
- `Union[int, str]`: Кортеж, содержащий количество токенов (int) и список закодированных токенов (str).

**Вызывает исключения**:
- Отсутствуют явные исключения, но могут возникнуть исключения, связанные с работой библиотеки `tiktoken`.

**Как работает функция**:

1. Функция принимает текст и название модели в качестве аргументов.
2. Использует библиотеку `tiktoken` для получения кодировщика для указанной модели.
3. Кодирует текст с помощью полученного кодировщика.
4. Определяет количество токенов в закодированном тексте.
5. Возвращает количество токенов и список закодированных токенов.

```
   Начало
     ↓
Получение кодировщика для модели
     ↓
    Кодирование текста
     ↓
  Определение количества токенов
     ↓
  Возврат количества и списка токенов
     ↓
    Конец
```

**Примеры**:

```python
# Пример 1: Токенизация текста с использованием модели по умолчанию
text = "Пример текста для токенизации."
tokens, encoded = tokenize(text)
print(f"Количество токенов: {tokens}")
print(f"Закодированные токены: {encoded}")

# Пример 2: Токенизация текста с указанием конкретной модели
text = "Another example for tokenization."
tokens, encoded = tokenize(text, model='gpt-3.5-turbo')
print(f"Количество токенов: {tokens}")
print(f"Закодированные токены: {encoded}")