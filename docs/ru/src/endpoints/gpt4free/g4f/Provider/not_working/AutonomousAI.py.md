# Модуль `AutonomousAI`

## Обзор

Модуль `AutonomousAI` предоставляет асинхронный генератор для взаимодействия с различными моделями AI через API сервиса autonomous.ai. Он поддерживает потоковую передачу данных и системные сообщения, а также предоставляет возможность использования истории сообщений. Модуль включает в себя функциональность для кодирования и декодирования сообщений в формате base64 для передачи через API.

## Подробней

Этот модуль является частью проекта `hypotez` и предназначен для интеграции с сервисом autonomous.ai, который предоставляет доступ к различным AI-моделям, таким как `llama`, `qwen_coder`, `hermes`, `vision` и `summary`. Он использует асинхронные запросы для взаимодействия с API и предоставляет результаты в виде асинхронного генератора. Это позволяет эффективно обрабатывать большие объемы данных и обеспечивает потоковую передачу ответов от AI-моделей.

## Классы

### `AutonomousAI`

**Описание**: Класс `AutonomousAI` является асинхронным генератором провайдера и миксином для работы с моделями.

**Наследует**:
- `AsyncGeneratorProvider`: Обеспечивает базовую функциональность для асинхронных генераторов провайдеров.
- `ProviderModelMixin`: Предоставляет функциональность для работы с моделями провайдера.

**Атрибуты**:
- `url` (str): URL сервиса autonomous.ai.
- `api_endpoints` (dict): Словарь, содержащий URL-адреса для различных моделей AI.
- `working` (bool): Флаг, указывающий, работает ли провайдер в данный момент.
- `supports_stream` (bool): Флаг, указывающий, поддерживает ли провайдер потоковую передачу данных.
- `supports_system_message` (bool): Флаг, указывающий, поддерживает ли провайдер системные сообщения.
- `supports_message_history` (bool): Флаг, указывающий, поддерживает ли провайдер историю сообщений.
- `default_model` (str): Модель, используемая по умолчанию.
- `models` (List[str]): Список поддерживаемых моделей.
- `model_aliases` (dict): Словарь, содержащий псевдонимы моделей.

**Методы**:
- `create_async_generator`: Создает асинхронный генератор для взаимодействия с API.

## Функции

### `create_async_generator`

```python
@classmethod
async def create_async_generator(
    cls,
    model: str,
    messages: Messages,
    proxy: str = None,
    stream: bool = False,
    **kwargs
) -> AsyncResult:
    """
    Создает асинхронный генератор для взаимодействия с API autonomous.ai.

    Args:
        cls (AutonomousAI): Класс `AutonomousAI`.
        model (str): Название модели AI для использования.
        messages (Messages): Список сообщений для отправки в API.
        proxy (str, optional): URL прокси-сервера для использования. По умолчанию `None`.
        stream (bool, optional): Флаг, указывающий, использовать ли потоковую передачу данных. По умолчанию `False`.
        **kwargs: Дополнительные аргументы.

    Returns:
        AsyncResult: Асинхронный генератор, возвращающий результаты от API.

    Raises:
        Exception: В случае возникновения ошибки при взаимодействии с API.

    """
```

**Назначение**: Создает асинхронный генератор для взаимодействия с API autonomous.ai и получения ответов от AI-модели.

**Параметры**:
- `cls` (AutonomousAI): Класс `AutonomousAI`.
- `model` (str): Название AI-модели для использования (например, "llama", "qwen_coder").
- `messages` (Messages): Список сообщений, отправляемых в API. Это структурированный список, представляющий собой историю общения с AI.
- `proxy` (str, optional): URL прокси-сервера для использования при подключении к API. По умолчанию `None`, что означает отсутствие прокси.
- `stream` (bool, optional): Флаг, указывающий, следует ли использовать потоковую передачу данных. Если `True`, ответы от API будут генерироваться по частям. По умолчанию `False`.
- `**kwargs`: Дополнительные аргументы, которые могут быть переданы в API.

**Возвращает**:
- `AsyncResult`: Асинхронный генератор, который выдает результаты от API. Каждый результат представляет собой часть ответа AI-модели.

**Вызывает исключения**:
- `Exception`: Может возникать при проблемах с подключением к API, неправильном формате данных или других непредвиденных ситуациях.

**Как работает функция**:

1. **Определение эндпоинта API**: На основе выбранной модели (`model`) определяется соответствующий эндпоинт API из словаря `api_endpoints`.
2. **Формирование заголовков запроса**: Создаются HTTP-заголовки, необходимые для запроса к API, включая User-Agent, тип контента и т. д.
3. **Создание сессии aiohttp**: Используется `aiohttp.ClientSession` для выполнения асинхронных HTTP-запросов.
4. **Кодирование сообщений**: Список сообщений (`messages`) преобразуется в JSON-формат и кодируется в base64.
5. **Формирование данных запроса**: Создается словарь `data`, содержащий закодированные сообщения, идентификатор потока, флаг потоковой передачи и идентификатор AI-агента.
6. **Отправка POST-запроса**: Отправляется асинхронный POST-запрос к API с использованием `session.post`.
7. **Обработка потоковых данных**: Если включена потоковая передача (`stream=True`), функция итерируется по частям ответа (`chunk`) и извлекает содержимое.
8. **Декодирование и фильтрация данных**: Каждая часть ответа декодируется, удаляется префикс "data: ", и извлекается содержимое из JSON.
9. **Генерация результатов**: Функция `yield` возвращает извлеченное содержимое в виде части ответа AI-модели. Если обнаружена причина завершения (`finish_reason`), она также возвращается.
10. **Обработка ошибок JSON**: В случае ошибки при декодировании JSON, она игнорируется и происходит переход к следующей части ответа.

```
Определение эндпоинта API и заголовков
│
→ Создание сессии aiohttp
│
Кодирование сообщений в JSON и base64
│
Формирование данных запроса
│
→ Отправка POST-запроса к API
│
Обработка потоковых данных (если stream=True)
│
Декодирование и фильтрация данных
│
→ Генерация результатов (yield)
│
Обработка ошибок JSON
↓
Конец
```

**Примеры**:

1. **Простой пример использования с моделью llama**:

```python
import asyncio
from typing import AsyncGenerator, List, Dict, Any

from src.endpoints.gpt4free.g4f.Provider.not_working.AutonomousAI import AutonomousAI

async def main():
    messages: List[Dict[str, str]] = [
        {"role": "user", "content": "Напиши Hello World на Python"}
    ]
    generator: AsyncGenerator[Any, None] = AutonomousAI.create_async_generator(model="llama", messages=messages)
    async for message in generator:
        print(message, end="")

if __name__ == "__main__":
    asyncio.run(main())
```

2. **Использование с прокси и потоковой передачей**:

```python
import asyncio
from typing import AsyncGenerator, List, Dict, Any

from src.endpoints.gpt4free.g4f.Provider.not_working.AutonomousAI import AutonomousAI

async def main():
    messages: List[Dict[str, str]] = [
        {"role": "user", "content": "Расскажи о Python"}
    ]
    proxy = "http://your_proxy:8080"
    generator: AsyncGenerator[Any, None] = AutonomousAI.create_async_generator(model="llama", messages=messages, proxy=proxy, stream=True)
    async for message in generator:
        print(message, end="")

if __name__ == "__main__":
    asyncio.run(main())
```