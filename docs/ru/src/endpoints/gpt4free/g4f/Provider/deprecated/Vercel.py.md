## Модуль `Vercel`

Модуль предоставляет интерфейс для взаимодействия с моделями, размещенными на платформе Vercel AI. Он позволяет создавать запросы к моделям, поддерживая стриминг ответов и использование прокси.

## Обзор

Модуль `Vercel` является частью подсистемы `gpt4free` проекта `hypotez` и предназначен для обеспечения возможности работы с различными моделями ИИ через API Vercel. Он включает в себя функциональность для формирования запросов к моделям, обработки ответов и управления параметрами подключения.

## Подробнее

Модуль обеспечивает абстракцию для взаимодействия с API Vercel, скрывая детали реализации отправки запросов и получения ответов. Он поддерживает как обычные запросы, так и стриминг ответов, что позволяет получать данные в реальном времени по мере их генерации моделью.

## Классы

### `Vercel`

**Описание**: Класс `Vercel` представляет собой провайдер для работы с моделями Vercel AI.

**Наследует**:

- `AbstractProvider`: Абстрактный базовый класс для провайдеров, определяющий основной интерфейс взаимодействия с моделями.

**Атрибуты**:

- `url` (str): URL-адрес API Vercel.
- `working` (bool): Флаг, указывающий на работоспособность провайдера (в данном случае всегда `False`).
- `supports_message_history` (bool): Флаг, указывающий, поддерживает ли провайдер историю сообщений (в данном случае `True`).
- `supports_gpt_35_turbo` (bool): Флаг, указывающий, поддерживает ли провайдер модель `gpt-3.5-turbo` (в данном случае `True`).
- `supports_stream` (bool): Флаг, указывающий, поддерживает ли провайдер потоковую передачу данных (в данном случае `True`).

**Методы**:

- `create_completion(model: str, messages: Messages, stream: bool, proxy: str = None, **kwargs) -> CreateResult`:
  Метод для создания запроса к модели Vercel и получения результата.
- `get_anti_bot_token() -> str`: Метод для получения anti-bot токена, необходимого для аутентификации запросов.

### `ModelInfo`

**Описание**: `ModelInfo` - это класс `TypedDict`, используемый для определения структуры данных, содержащих информацию о модели.

**Атрибуты**:

- `id` (str): Идентификатор модели.
- `default_params` (dict[str, Any]): Словарь с параметрами по умолчанию для модели.

### `model_info`

**Описание**: `model_info` - это словарь, содержащий информацию о различных моделях, поддерживаемых провайдером Vercel. Ключами словаря являются имена моделей, а значениями - объекты типа `ModelInfo`, содержащие идентификатор модели и параметры по умолчанию.

## Методы класса

### `create_completion`

```python
    @staticmethod
    def create_completion(
        model: str,
        messages: Messages,
        stream: bool,
        proxy: str = None,
        **kwargs
    ) -> CreateResult:
        """
        Создает запрос к модели Vercel и возвращает результат.

        Args:
            model (str): Имя модели для запроса.
            messages (Messages): Список сообщений для отправки в модель.
            stream (bool): Флаг, указывающий, использовать ли потоковую передачу данных.
            proxy (str, optional): Адрес прокси-сервера для использования. По умолчанию `None`.
            **kwargs: Дополнительные параметры для передачи в модель.

        Returns:
            CreateResult: Результат выполнения запроса.

        Raises:
            MissingRequirementsError: Если не установлен пакет `PyExecJS`.
            ValueError: Если указанная модель не поддерживается Vercel.

        Как работает функция:
        - Проверяет наличие необходимых зависимостей (`PyExecJS`).
        - Если зависимости отсутствуют, вызывает исключение `MissingRequirementsError`.
        - Проверяет, указана ли модель. Если нет, использует `gpt-3.5-turbo` по умолчанию.
        - Если указанная модель не найдена в списке поддерживаемых, вызывает исключение `ValueError`.
        - Формирует заголовки запроса, включая anti-bot токен, полученный с помощью функции `get_anti_bot_token`.
        - Формирует данные запроса в формате JSON, включая идентификатор модели, сообщения и параметры.
        - Отправляет POST-запрос к API Vercel (`https://chat.vercel.ai/api/chat`) с использованием библиотеки `requests`.
        - Обрабатывает ответ от API, возвращая токены с использованием `yield`.
        - В случае ошибки повторяет запрос до `max_retries` раз.

        Внутренние функции:
        - Отсутствуют.

        """
```

**Примеры**:

```python
# Пример использования create_completion
# model = "gpt-3.5-turbo"
# messages = [{"role": "user", "content": "Hello, Vercel!"}]
# stream = True
# proxy = "http://your-proxy:8080"
# result = Vercel.create_completion(model=model, messages=messages, stream=stream, proxy=proxy)
# for token in result:
#     print(token, end="")
```

### `get_anti_bot_token`

```python
def get_anti_bot_token() -> str:
    """
    Получает anti-bot токен, необходимый для аутентификации запросов.

    Returns:
        str: Anti-bot токен.

    Как работает функция:
        - Функция отправляет GET-запрос к `https://sdk.vercel.ai/openai.jpeg` для получения данных, необходимых для генерации токена.
        - Извлекает содержимое ответа и декодирует его из base64.
        - Формирует JavaScript-скрипт, который выполняет функцию извлеченную из ответа.
        - Использует `execjs` для выполнения JavaScript-скрипта и получения токена.
        - Формирует JSON-структуру с токеном и дополнительной информацией.
        - Кодирует JSON-структуру в base64 и возвращает результат.

    """
```

**Примеры**:

```python
# Пример использования get_anti_bot_token
# token = get_anti_bot_token()
# print(f"Anti-bot token: {token}")
```

## Параметры класса

- `url` (str): URL-адрес API Vercel.
- `working` (bool): Флаг, указывающий на работоспособность провайдера.
- `supports_message_history` (bool): Флаг, указывающий, поддерживает ли провайдер историю сообщений.
- `supports_gpt_35_turbo` (bool): Флаг, указывающий, поддерживает ли провайдер модель `gpt-3.5-turbo`.
- `supports_stream` (bool): Флаг, указывающий, поддерживает ли провайдер потоковую передачу данных.

## Примеры

```python
# Пример создания запроса к модели Vercel
# from src.endpoints.gpt4free.g4f.Provider.deprecated.Vercel import Vercel
# model = "gpt-3.5-turbo"
# messages = [{"role": "user", "content": "Hello, Vercel!"}]
# stream = True
# proxy = "http://your-proxy:8080"
# result = Vercel.create_completion(model=model, messages=messages, stream=stream, proxy=proxy)
# for token in result:
#     print(token, end="")