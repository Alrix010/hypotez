# Модуль для работы с локальными моделями GPT4All

## Обзор

Модуль `Local.py` предоставляет интерфейс для взаимодействия с локально установленными моделями GPT4All. Он позволяет использовать эти модели для создания завершений текста, поддерживая как потоковый, так и не потоковый режимы. Модуль интегрируется с фреймворком `g4f`, обеспечивая абстракцию от конкретной реализации локального провайдера моделей.

## Подробней

Этот модуль является частью проекта `hypotez` и предназначен для интеграции локальных моделей GPT4All в систему. Он предоставляет класс `Local`, который наследуется от `AbstractProvider` и `ProviderModelMixin`, что позволяет использовать его как провайдера моделей в рамках фреймворка `g4f`. Модуль обрабатывает случаи, когда необходимые зависимости не установлены, и предоставляет инструкции по их установке.

## Классы

### `Local(AbstractProvider, ProviderModelMixin)`

**Описание**: Класс `Local` представляет собой провайдера локальных моделей GPT4All. Он отвечает за загрузку списка доступных моделей и создание завершений текста с использованием этих моделей.

**Наследует**:
- `AbstractProvider`: Предоставляет базовый интерфейс для всех провайдеров моделей.
- `ProviderModelMixin`: Предоставляет функциональность для работы с моделями, специфичную для провайдера.

**Атрибуты**:
- `label` (str): Метка провайдера, `"GPT4All"`.
- `working` (bool): Указывает, что провайдер находится в рабочем состоянии (`True`).
- `supports_message_history` (bool): Указывает, что провайдер поддерживает историю сообщений (`True`).
- `supports_system_message` (bool): Указывает, что провайдер поддерживает системные сообщения (`True`).
- `supports_stream` (bool): Указывает, что провайдер поддерживает потоковый режим (`True`).
- `models` (List[str]): Список доступных моделей.
- `default_model` (str): Модель по умолчанию.

**Методы**:
- `get_models()`: Возвращает список доступных моделей.
- `create_completion(model: str, messages: Messages, stream: bool, **kwargs) -> CreateResult`: Создает завершение текста с использованием указанной модели.

## Функции

### `get_models()`

```python
@classmethod
def get_models(cls):
    """Получает список доступных локальных моделей GPT4All.

    Args:
        cls: Ссылка на класс Local.

    Returns:
        List[str]: Список доступных моделей.

    Как работает функция:
    1. Проверяет, загружен ли уже список моделей в атрибуте `cls.models`.
    2. Если список моделей не загружен, вызывает функцию `get_models()` из модуля `...locals.models` для получения списка.
    3. Устанавливает атрибут `cls.models` равным полученному списку моделей.
    4. Устанавливает атрибут `cls.default_model` равным первому элементу списка моделей.
    5. Возвращает список доступных моделей.

    ASCII flowchart:
    Проверка загруженности моделей --> Загрузка моделей из ...locals.models --> Установка атрибутов класса --> Возврат списка моделей

    Примеры:
    >>> Local.get_models()
    ['ggml-model-gpt4all-falcon-q4_0.bin', 'ggml-model-gpt4all-mpt-30b-chat.bin']
    """
    ...
```

### `create_completion(model: str, messages: Messages, stream: bool, **kwargs) -> CreateResult`

```python
@classmethod
def create_completion(
    cls,
    model: str,
    messages: Messages,
    stream: bool,
    **kwargs
) -> CreateResult:
    """Создает завершение текста с использованием локальной модели GPT4All.

    Args:
        cls: Ссылка на класс Local.
        model (str): Имя используемой модели.
        messages (Messages): Список сообщений для создания завершения.
        stream (bool): Флаг, указывающий, следует ли использовать потоковый режим.
        **kwargs: Дополнительные аргументы, передаваемые в функцию создания завершения.

    Returns:
        CreateResult: Результат создания завершения текста.

    Raises:
        MissingRequirementsError: Если не установлены необходимые зависимости (пакет `gpt4all`).

    Как работает функция:
    1. Проверяет, установлены ли необходимые зависимости (пакет `gpt4all`).
    2. Если зависимости не установлены, вызывает исключение `MissingRequirementsError` с инструкцией по установке.
    3. Если зависимости установлены, вызывает функцию `create_completion()` из модуля `...locals.provider` для создания завершения текста.
    4. Передает имя модели, список сообщений, флаг потокового режима и дополнительные аргументы в функцию `create_completion()`.
    5. Возвращает результат создания завершения текста.

    ASCII flowchart:
    Проверка зависимостей --> Вызов LocalProvider.create_completion --> Возврат результата

    Примеры:
    >>> messages = [{"role": "user", "content": "Напиши короткий рассказ о котике."}]
    >>> Local.create_completion(model='ggml-model-gpt4all-falcon-q4_0.bin', messages=messages, stream=False)
    'Котик сидел на окошке и смотрел на улицу...'
    """
    ...