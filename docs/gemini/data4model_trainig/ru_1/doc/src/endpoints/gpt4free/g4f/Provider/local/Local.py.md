# Модуль для работы с локальными моделями GPT4All

## Обзор

Модуль `Local.py` предоставляет интерфейс для использования локальных моделей GPT4All в проекте `hypotez`. Он позволяет генерировать ответы на основе предоставленных сообщений, поддерживая потоковую передачу и историю сообщений. Модуль интегрируется с `LocalProvider` и использует его для фактического создания завершений.

## Подробней

Этот модуль предоставляет класс `Local`, который является адаптером для локально развернутых моделей GPT4All. Он проверяет наличие необходимых зависимостей и использует `LocalProvider` для генерации ответов. Модуль также предоставляет метод для получения списка доступных локальных моделей.

## Классы

### `Local`

**Описание**: Класс для взаимодействия с локальными моделями GPT4All.

**Наследует**:
- `AbstractProvider`: Предоставляет базовый интерфейс для провайдеров моделей.
- `ProviderModelMixin`: Добавляет функциональность для работы с моделями провайдера.

**Атрибуты**:
- `label` (str): Метка провайдера, в данном случае "GPT4All".
- `working` (bool): Указывает, работает ли провайдер, в данном случае `True`.
- `supports_message_history` (bool): Указывает, поддерживает ли провайдер историю сообщений, в данном случае `True`.
- `supports_system_message` (bool): Указывает, поддерживает ли провайдер системные сообщения, в данном случае `True`.
- `supports_stream` (bool): Указывает, поддерживает ли провайдер потоковую передачу, в данном случае `True`.
- `models` (list): Список доступных локальных моделей.
- `default_model` (str): Модель по умолчанию.

**Принцип работы**:
Класс `Local` инициализируется как адаптер для локальных моделей GPT4All. При первом вызове `get_models` он получает список доступных моделей с помощью функции `get_models()` из модуля `...locals.models`. Если необходимые зависимости не установлены, `create_completion` вызывает исключение `MissingRequirementsError`. В противном случае он использует `LocalProvider` для создания завершения на основе выбранной модели и предоставленных сообщений.

**Методы**:

- `get_models`: Возвращает список доступных локальных моделей.
- `create_completion`: Создает завершение на основе предоставленных параметров.

## Методы класса

### `get_models`

```python
    @classmethod
    def get_models(cls) -> list[str]:
        """
        Возвращает список доступных локальных моделей.

        Если список моделей еще не был инициализирован, он получает его с помощью функции `get_models()` из модуля `...locals.models`.

        Returns:
            list[str]: Список доступных локальных моделей.
        """
```

**Как работает функция**:

Функция `get_models` является методом класса `Local`. При первом вызове она проверяет, был ли уже инициализирован список моделей (`cls.models`). Если нет, она вызывает функцию `get_models()` из модуля `...locals.models` для получения списка доступных локальных моделей и сохраняет его в `cls.models`. Затем функция возвращает этот список.

**Примеры**:

```python
models = Local.get_models()
print(models)
```

### `create_completion`

```python
    @classmethod
    def create_completion(
        cls,
        model: str,
        messages: Messages,
        stream: bool,
        **kwargs
    ) -> CreateResult:
        """
        Создает завершение на основе предоставленных параметров.

        Args:
            model (str): Название модели для использования.
            messages (Messages): Список сообщений для передачи модели.
            stream (bool): Указывает, следует ли использовать потоковую передачу.
            **kwargs: Дополнительные аргументы для передачи в `LocalProvider.create_completion`.

        Returns:
            CreateResult: Результат создания завершения.

        Raises:
            MissingRequirementsError: Если не установлены необходимые зависимости (`gpt4all`).
        """
```

**Как работает функция**:

Функция `create_completion` является методом класса `Local`. Она принимает название модели, список сообщений, флаг потоковой передачи и дополнительные аргументы. Сначала она проверяет, установлены ли необходимые зависимости (`gpt4all`). Если нет, она вызывает исключение `MissingRequirementsError`. В противном случае она вызывает метод `create_completion` из `LocalProvider`, передавая ему выбранную модель, сообщения, флаг потоковой передачи и дополнительные аргументы. Результат этого вызова возвращается.

**Примеры**:

```python
try:
    result = Local.create_completion(model="ggml-model-gpt4all-falcon-q4_0.bin", messages=[{"role": "user", "content": "Hello"}], stream=False)
    print(result)
except MissingRequirementsError as ex:
    print(f"Error: {ex}")
```

## Параметры класса

- `label` (str): Метка провайдера, в данном случае "GPT4All".
- `working` (bool): Указывает, работает ли провайдер, в данном случае `True`.
- `supports_message_history` (bool): Указывает, поддерживает ли провайдер историю сообщений, в данном случае `True`.
- `supports_system_message` (bool): Указывает, поддерживает ли провайдер системные сообщения, в данном случае `True`.
- `supports_stream` (bool): Указывает, поддерживает ли провайдер потоковую передачу, в данном случае `True`.
- `models` (list): Список доступных локальных моделей.
- `default_model` (str): Модель по умолчанию.

**Примеры**:

```python
print(f"Label: {Local.label}")
print(f"Supports stream: {Local.supports_stream}")