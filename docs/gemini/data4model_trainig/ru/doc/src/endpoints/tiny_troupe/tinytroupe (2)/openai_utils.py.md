# Модуль для работы с OpenAI API
=====================================

Модуль предоставляет утилиты для взаимодействия с OpenAI API, включая кэширование запросов, обработку ошибок и поддержку различных моделей. Он включает в себя классы для управления запросами к API и обработки ответов, а также поддержку Azure OpenAI Service.

## Обзор

Модуль предназначен для упрощения работы с OpenAI API, предоставляя удобные инструменты для отправки запросов, обработки ответов и управления кэшированием. Он также поддерживает различные типы моделей и предоставляет возможность настройки параметров API.

## Подробней

Модуль содержит классы и функции для взаимодействия с OpenAI API. Он позволяет отправлять запросы к API, обрабатывать ответы и управлять кэшированием запросов. Модуль также поддерживает различные типы моделей, такие как OpenAI и Azure OpenAI Service.

## Содержание

1.  [Основные принципы](#Основные-принципы)
2.  [Конфигурация](#Конфигурация)
3.  [Классы](#Классы)
    *   [LLMRequest](#LLMRequest)
    *   [LLMScalarWithJustificationResponse](#LLMScalarWithJustificationResponse)
    *   [OpenAIClient](#OpenAIClient)
    *   [AzureClient](#AzureClient)
4.  [Исключения](#Исключения)
    *   [InvalidRequestError](#InvalidRequestError)
    *   [NonTerminalError](#NonTerminalError)
5.  [Функции](#Функции)
    *   [register_client](#register_client)
    *   [_get_client_for_api_type](#_get_client_for_api_type)
    *   [client](#client)
    *   [force_api_type](#force_api_type)
    *   [force_api_cache](#force_api_cache)

## Основные принципы

Модуль разработан с учетом следующих принципов:

*   **Удобство использования**: Предоставляет простые и понятные интерфейсы для взаимодействия с OpenAI API.
*   **Гибкость**: Поддерживает различные типы моделей и предоставляет возможность настройки параметров API.
*   **Кэширование**: Обеспечивает кэширование запросов для повышения производительности и снижения затрат.
*   **Обработка ошибок**: Предоставляет механизмы для обработки ошибок и повторных попыток.

## Конфигурация

Модуль использует файл `config.ini` для настройки параметров API. В частности, в секции `OpenAI` можно указать следующие параметры:

*   `MODEL`: ID модели для использования (например, "gpt-4o").
*   `MAX_TOKENS`: Максимальное количество токенов в ответе.
*   `TEMPERATURE`: Температура для контроля "креативности" ответа.
*   `TOP_P`: Параметр для контроля "качества" ответа.
*   `FREQ_PENALTY`: Штраф за частоту для контроля "повторения" ответа.
*   `PRESENCE_PENALTY`: Штраф за присутствие для контроля "разнообразия" ответа.
*   `TIMEOUT`: Максимальное время ожидания ответа от API.
*   `MAX_ATTEMPTS`: Максимальное количество попыток отправки запроса.
*   `WAITING_TIME`: Время ожидания между запросами.
*   `EXPONENTIAL_BACKOFF_FACTOR`: Фактор экспоненциального увеличения времени ожидания между запросами.
*   `EMBEDDING_MODEL`: Модель для получения эмбеддингов текста.
*   `CACHE_API_CALLS`: Включает или отключает кэширование API-запросов.
*   `CACHE_FILE_NAME`: Имя файла для хранения кэша API-запросов.
*   `API_TYPE`: Тип используемого API ("openai" или "azure").
*   `AZURE_API_VERSION`: Версия API для Azure OpenAI Service.
*   `AZURE_OPENAI_ENDPOINT`: Эндпоинт для Azure OpenAI Service.
*   `AZURE_OPENAI_KEY`: Ключ API для Azure OpenAI Service.

## Классы

### `LLMRequest`

**Описание**: Класс, представляющий запрос к языковой модели (LLM). Он содержит входные сообщения, конфигурацию модели и вывод модели.

**Атрибуты**:

*   `system_template_name` (str): Имя шаблона системного сообщения.
*   `user_template_name` (str): Имя шаблона пользовательского сообщения.
*   `system_prompt` (str): Системное сообщение.
*   `user_prompt` (str): Пользовательское сообщение.
*   `output_type` (type): Тип ожидаемого вывода.
*   `model_params` (dict): Параметры модели.
*   `model_output` (str): Вывод модели.
*   `messages` (list): Список сообщений для отправки в модель.
*   `response_raw` (str): Необработанный ответ от модели.
*   `response_json` (dict): JSON-представление ответа от модели.
*   `response_value` (Any): Значение, извлеченное из ответа.
*   `response_justification` (str): Обоснование значения, извлеченного из ответа.
*   `response_confidence` (float): Уровень уверенности в значении и обосновании.

**Методы**:

*   `__init__(system_template_name: str = None, system_prompt: str = None, user_template_name: str = None, user_prompt: str = None, output_type=None, \*\*model_params)`:
    *   **Назначение**: Инициализирует экземпляр класса `LLMRequest`.
    *   **Параметры**:
        *   `system_template_name` (str, optional): Имя шаблона системного сообщения. По умолчанию `None`.
        *   `system_prompt` (str, optional): Системное сообщение. По умолчанию `None`.
        *   `user_template_name` (str, optional): Имя шаблона пользовательского сообщения. По умолчанию `None`.
        *   `user_prompt` (str, optional): Пользовательское сообщение. По умолчанию `None`.
        *   `output_type` (type, optional): Тип ожидаемого вывода. По умолчанию `None`.
        *   `model_params` (dict, optional): Параметры модели.
    *   **Возвращает**: `None`
    *   **Вызывает исключения**:
        *   `ValueError`: Если указаны и шаблон, и сообщение одновременно, или если не указаны ни шаблон, ни сообщение.
*   `call(**rendering_configs)`:
    *   **Назначение**: Вызывает языковую модель с указанными параметрами.
    *   **Параметры**:
        *   `rendering_configs` (dict): Конфигурации рендеринга (переменные шаблона) для составления начальных сообщений.
    *   **Возвращает**: Содержимое ответа модели.
    *   **Внутренние функции**:
        *   `_coerce_to_bool(llm_output)`:
            *   **Назначение**: Преобразует вывод LLM в логическое значение.
            *   **Параметры**:
                *   `llm_output` (str | bool): Вывод LLM для преобразования.
            *   **Возвращает**: Логическое значение вывода LLM.
            *   **Вызывает исключения**:
                *   `ValueError`: Если вывод LLM не содержит распознаваемое логическое значение.
        *   `_request_bool_llm_message()`:
            *   **Назначение**: Создает сообщение для запроса логического значения от LLM.
            *   **Параметры**: `None`
            *   **Возвращает**: Словарь, представляющий сообщение для запроса логического значения.
        *   `_coerce_to_integer(llm_output: str)`:
            *   **Назначение**: Преобразует вывод LLM в целое число.
            *   **Параметры**:
                *   `llm_output` (str): Вывод LLM для преобразования.
            *   **Возвращает**: Целочисленное значение вывода LLM.
            *   **Вызывает исключения**:
                *   `ValueError`: Если вывод LLM не содержит распознаваемое целочисленное значение.
        *   `_request_integer_llm_message()`:
            *   **Назначение**: Создает сообщение для запроса целочисленного значения от LLM.
            *   **Параметры**: `None`
            *   **Возвращает**: Словарь, представляющий сообщение для запроса целочисленного значения.
        *   `_coerce_to_float(llm_output: str)`:
            *   **Назначение**: Преобразует вывод LLM в число с плавающей точкой.
            *   **Параметры**:
                *   `llm_output` (str): Вывод LLM для преобразования.
            *   **Возвращает**: Значение с плавающей точкой вывода LLM.
            *   **Вызывает исключения**:
                *   `ValueError`: Если вывод LLM не содержит распознаваемое значение с плавающей точкой.
        *   `_request_float_llm_message()`:
            *   **Назначение**: Создает сообщение для запроса значения с плавающей точкой от LLM.
            *   **Параметры**: `None`
            *   **Возвращает**: Словарь, представляющий сообщение для запроса значения с плавающей точкой.
        *   `_coerce_to_enumerable(llm_output: str, options: list)`:
            *   **Назначение**: Преобразует вывод LLM в один из указанных вариантов.
            *   **Параметры**:
                *   `llm_output` (str): Вывод LLM для преобразования.
                *   `options` (list): Список рассматриваемых вариантов.
            *   **Возвращает**: Значение варианта вывода LLM.
            *   **Вызывает исключения**:
                *   `ValueError`: Если вывод LLM не содержит распознаваемое значение варианта.
        *   `_request_enumerable_llm_message(options: list)`:
            *   **Назначение**: Создает сообщение для запроса перечислимого значения от LLM.
            *   **Параметры**:
                *   `options` (list): Список рассматриваемых вариантов.
            *   **Возвращает**: Словарь, представляющий сообщение для запроса перечислимого значения.

    *   **Как работает функция**:
        1.  Определяет, использовать ли шаблоны или прямые подсказки для составления сообщений.
        2.  Настраивает типизацию для вывода, добавляя инструкции для LLM о формате ответа (JSON с полями `value`, `justification` и `confidence`).
        3.  Вызывает LLM модель с сформированными сообщениями и параметрами.
        4.  Извлекает и преобразует результат в соответствии с указанным типом вывода (bool, int, float, list, str).
        5.  Возвращает преобразованное значение.
    *   **Примеры**:

        ```python
        # Пример использования с шаблонами
        request = LLMRequest(system_template_name="system_template", user_template_name="user_template", output_type=bool, model_params={"model": "gpt-4"})
        result = request(param1="value1", param2="value2")

        # Пример использования с прямыми подсказками
        request = LLMRequest(system_prompt="System prompt", user_prompt="User prompt", output_type=int, model_params={"model": "gpt-4"})
        result = request()
        ```
*   `__repr__()`:
    *   **Назначение**: Возвращает строковое представление объекта `LLMRequest`.
    *   **Параметры**: `None`
    *   **Возвращает**: Строковое представление объекта.

### `LLMScalarWithJustificationResponse`

**Описание**: Класс для представления типизированного ответа от языковой модели (LLM) со скалярным значением, обоснованием и уровнем уверенности.

**Атрибуты**:

*   `value` (str | int | float | bool): Значение ответа.
*   `justification` (str): Обоснование или объяснение ответа.
*   `confidence` (float): Уровень уверенности в ответе.

### `OpenAIClient`

**Описание**: Утилитный класс для взаимодействия с OpenAI API.

**Атрибуты**:

*   `cache_api_calls` (bool): Флаг, указывающий, следует ли кэшировать вызовы API.
*   `cache_file_name` (str): Имя файла для хранения кэша API-вызовов.
*   `api_cache` (dict): Кэш API-вызовов.
*   `client` (OpenAI): Клиент OpenAI.

**Методы**:

*   `__init__(cache_api_calls=default["cache_api_calls"], cache_file_name=default["cache_file_name"])`:
    *   **Назначение**: Инициализирует экземпляр класса `OpenAIClient`.
    *   **Параметры**:
        *   `cache_api_calls` (bool, optional): Флаг, указывающий, следует ли кэшировать вызовы API. По умолчанию значение берется из `default["cache_api_calls"]`.
        *   `cache_file_name` (str, optional): Имя файла для хранения кэша API-вызовов. По умолчанию значение берется из `default["cache_file_name"]`.
    *   **Возвращает**: `None`
*   `set_api_cache(cache_api_calls, cache_file_name=default["cache_file_name"])`:
    *   **Назначение**: Включает или отключает кэширование API-вызовов.
    *   **Параметры**:
        *   `cache_api_calls` (bool): Флаг, указывающий, следует ли кэшировать вызовы API.
        *   `cache_file_name` (str, optional): Имя файла для хранения кэша API-вызовов. По умолчанию значение берется из `default["cache_file_name"]`.
    *   **Возвращает**: `None`
*   `send_message(current_messages, model=default["model"], temperature=default["temperature"], max_tokens=default["max_tokens"], top_p=default["top_p"], frequency_penalty=default["frequency_penalty"], presence_penalty=default["presence_penalty"], stop=[], timeout=default["timeout"], max_attempts=default["max_attempts"], waiting_time=default["waiting_time"], exponential_backoff_factor=default["exponential_backoff_factor"], n=1, response_format=None, echo=False)`:
    *   **Назначение**: Отправляет сообщение в OpenAI API и возвращает ответ.
    *   **Параметры**:
        *   `current_messages` (list): Список словарей, представляющих историю разговора.
        *   `model` (str, optional): ID модели для использования для генерации ответа. По умолчанию значение берется из `default["model"]`.
        *   `temperature` (float, optional): Управляет "креативностью" ответа. Более высокие значения приводят к более разнообразным ответам. По умолчанию значение берется из `default["temperature"]`.
        *   `max_tokens` (int, optional): Максимальное количество токенов (слов или знаков препинания) для генерации в ответе. По умолчанию значение берется из `default["max_tokens"]`.
        *   `top_p` (float, optional): Управляет "качеством" ответа. Более высокие значения приводят к более связным ответам. По умолчанию значение берется из `default["top_p"]`.
        *   `frequency_penalty` (float, optional): Управляет "повторением" ответа. Более высокие значения приводят к меньшему повторению. По умолчанию значение берется из `default["frequency_penalty"]`.
        *   `presence_penalty` (float, optional): Управляет "разнообразием" ответа. Более высокие значения приводят к более разнообразным ответам. По умолчанию значение берется из `default["presence_penalty"]`.
        *   `stop` (str, optional): Строка, которая, если она встречается в сгенерированном ответе, приводит к прекращению генерации. По умолчанию [].
        *   `timeout` (int, optional): Максимальное количество секунд ожидания ответа от API. По умолчанию значение берется из `default["timeout"]`.
        *   `max_attempts` (int, optional): Максимальное количество попыток, прежде чем отказаться от создания ответа. По умолчанию значение берется из `default["max_attempts"]`.
        *   `waiting_time` (int, optional): Количество секунд ожидания между запросами. По умолчанию значение берется из `default["waiting_time"]`.
        *   `exponential_backoff_factor` (int, optional): Коэффициент, на который следует увеличивать время ожидания между запросами. По умолчанию значение берется из `default["exponential_backoff_factor"]`.
        *   `n` (int, optional): Количество завершений для создания. По умолчанию 1.
        *   `response_format` (Any, optional): Формат ответа, если есть. По умолчанию `None`.
        *   `echo` (bool, optional): Должен ли запрос повторяться в ответе. По умолчанию `False`.
    *   **Возвращает**: Словарь, представляющий сгенерированный ответ.
    *   **Вызывает исключения**:
        *   `InvalidRequestError`: Если запрос к OpenAI API недействителен.
    *   **Внутренние функции**:
        *   `aux_exponential_backoff()`:
            *   **Назначение**: Выполняет экспоненциальную задержку, если запрос не удался.
            *   **Параметры**: `None`
            *   **Возвращает**: `None`

    *   **Как работает функция**:
        1.  Настраивает параметры OpenAI API.
        2.  Повторяет запрос до `max_attempts` раз.
        3.  Кэширует запросы и ответы, если включено кэширование.
        4.  Обрабатывает ошибки, такие как `RateLimitError` и `InvalidRequestError`.
        5.  Извлекает и возвращает полезные данные из ответа API.

    *   **Примеры**:

        ```python
        client = OpenAIClient()
        messages = [{"role": "user", "content": "Hello, how are you?"}]
        response = client.send_message(messages)
        print(response)
        ```
*   `get_embedding(text, model=default["embedding_model"])`:
    *   **Назначение**: Получает эмбеддинг заданного текста с использованием указанной модели.
    *   **Параметры**:
        *   `text` (str): Текст для получения эмбеддинга.
        *   `model` (str, optional): Имя модели для использования для эмбеддинга текста. По умолчанию значение берется из `default["embedding_model"]`.
    *   **Возвращает**: Эмбеддинг текста.
*   `_setup_from_config()`:
    *   **Назначение**: Устанавливает конфигурации OpenAI API для этого клиента.
    *   **Параметры**: `None`
    *   **Возвращает**: `None`
*   `_raw_model_call(model, chat_api_params)`:
    *   **Назначение**: Вызывает OpenAI API с заданными параметрами. Подклассы должны переопределить этот метод, чтобы реализовать свои собственные вызовы API.
    *   **Параметры**:
        *   `model` (str): ID модели для использования.
        *   `chat_api_params` (dict): Параметры для вызова API чата.
    *   **Возвращает**: Ответ от API.
*   `_raw_model_response_extractor(response)`:
    *   **Назначение**: Извлекает ответ из ответа API. Подклассы должны переопределить этот метод, чтобы реализовать свои собственные извлечения ответов.
    *   **Параметры**:
        *   `response` (dict): Ответ от API.
    *   **Возвращает**: Извлеченный ответ.
*   `_count_tokens(messages: list, model: str)`:
    *   **Назначение**: Подсчитывает количество токенов OpenAI в списке сообщений с использованием tiktoken.
    *   **Параметры**:
        *   `messages` (list): Список словарей, представляющих историю разговора.
        *   `model` (str): Имя модели для использования для кодирования строки.
    *   **Возвращает**: Количество токенов.
*   `_save_cache()`:
    *   **Назначение**: Сохраняет кэш API на диск. Мы используем pickle для этого, потому что некоторые объекты не сериализуемы в JSON.
    *   **Параметры**: `None`
    *   **Возвращает**: `None`
*   `_load_cache()`:
    *   **Назначение**: Загружает кэш API с диска.
    *   **Параметры**: `None`
    *   **Возвращает**: Кэш API.
*   `_raw_embedding_model_call(text, model)`:
    *   **Назначение**: Вызывает OpenAI API для получения эмбеддинга заданного текста. Подклассы должны переопределить этот метод, чтобы реализовать свои собственные вызовы API.
    *   **Параметры**:
        *   `text` (str): Текст для получения эмбеддинга.
        *   `model` (str): Имя модели для использования для эмбеддинга текста.
    *   **Возвращает**: Ответ от API.
*   `_raw_embedding_model_response_extractor(response)`:
    *   **Назначение**: Извлекает эмбеддинг из ответа API. Подклассы должны переопределить этот метод, чтобы реализовать свои собственные извлечения ответов.
    *   **Параметры**:
        *   `response` (dict): Ответ от API.
    *   **Возвращает**: Извлеченный эмбеддинг.

### `AzureClient`

**Описание**: Подкласс `OpenAIClient`, предназначенный для взаимодействия с Azure OpenAI Service API.

**Наследует**: `OpenAIClient`

**Методы**:

*   `__init__(cache_api_calls=default["cache_api_calls"], cache_file_name=default["cache_file_name"])`:
    *   **Назначение**: Инициализирует экземпляр класса `AzureClient`.
    *   **Параметры**:
        *   `cache_api_calls` (bool, optional): Флаг, указывающий, следует ли кэшировать вызовы API. По умолчанию значение берется из `default["cache_api_calls"]`.
        *   `cache_file_name` (str, optional): Имя файла для хранения кэша API-вызовов. По умолчанию значение берется из `default["cache_file_name"]`.
    *   **Возвращает**: `None`
*   `_setup_from_config()`:
    *   **Назначение**: Настраивает конфигурации Azure OpenAI Service API для этого клиента, включая конечную точку API и ключ.
    *   **Параметры**: `None`
    *   **Возвращает**: `None`

## Исключения

### `InvalidRequestError`

**Описание**: Исключение, возникающее, когда запрос к OpenAI API недействителен.

### `NonTerminalError`

**Описание**: Исключение, возникающее, когда происходит неуказанная ошибка, но мы знаем, что можем повторить попытку.

## Функции

### `register_client`

```python
def register_client(api_type, client) -> None:
    """
    Регистрирует клиент для заданного типа API.

    Args:
        api_type (str): Тип API, для которого мы хотим зарегистрировать клиент.
        client: Клиент для регистрации.
    """
```

### `_get_client_for_api_type`

```python
def _get_client_for_api_type(api_type) -> OpenAIClient | AzureClient:
    """
    Возвращает клиент для заданного типа API.

    Args:
        api_type (str): Тип API, для которого мы хотим получить клиент.

    Raises:
        ValueError: Если тип API не поддерживается.

    Returns:
        OpenAIClient | AzureClient: Клиент для заданного типа API.
    """
```

### `client`

```python
def client() -> OpenAIClient | AzureClient:
    """
    Возвращает клиент для настроенного типа API.
    """
```

### `force_api_type`

```python
def force_api_type(api_type) -> None:
    """
    Принудительно использует заданный тип API, тем самым переопределяя любую другую конфигурацию.

    Args:
        api_type (str): Тип API для использования.
    """
```

### `force_api_cache`

```python
def force_api_cache(cache_api_calls, cache_file_name=default["cache_file_name"]) -> None:
    """
    Принудительно использует заданную конфигурацию кэша API, тем самым переопределяя любую другую конфигурацию.

    Args:
        cache_api_calls (bool): Следует ли кэшировать вызовы API.
        cache_file_name (str): Имя файла для использования для кэширования вызовов API.
    """