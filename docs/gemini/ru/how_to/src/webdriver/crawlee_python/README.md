## Как использовать модуль Crawlee Python для автоматизации и сбора данных
=========================================================================================

Описание
-------------------------
Модуль `Crawlee Python` предоставляет пользовательскую реализацию `PlaywrightCrawler` с использованием библиотеки Crawlee. Он позволяет настраивать параметры запуска браузера, обрабатывать веб-страницы и извлекать из них данные. Управление конфигурацией осуществляется через файл `crawlee_python.json`.

Шаги выполнения
-------------------------
1. **Установка зависимостей**:
   - Убедитесь, что у вас установлены Python 3.x, Playwright и Crawlee.
   - Установите необходимые Python-пакеты с помощью команды:
     ```bash
     pip install playwright crawlee
     ```
   - Установите браузеры, необходимые для Playwright:
     ```bash
     playwright install
     ```

2. **Настройка конфигурации**:
   - Отредактируйте файл `crawlee_python.json`, чтобы настроить параметры краулера, такие как `max_requests`, `headless`, `browser_type`, `options`, `user_agent`, `proxy`, `viewport` и `timeout`.

3. **Использование модуля**:
   - Импортируйте класс `CrawleePython` из модуля `src.webdriver.crawlee_python`.
   - Инициализируйте `CrawleePython` с нужными параметрами.
   - Запустите краулер с помощью метода `run()`, передав список URL-адресов для обработки.

4. **Просмотр логов**:
   - Просматривайте логи, чтобы отслеживать ошибки, предупреждения и общую информацию о работе краулера.

Пример использования
-------------------------

```python
from src.webdriver.crawlee_python import CrawleePython
import asyncio

# Асинхронная функция для запуска краулера
async def main():
    # Инициализация CrawleePython с пользовательскими опциями
    crawler = CrawleePython(max_requests=10, headless=True, browser_type='chromium', options=["--headless"])
    # Запуск краулера с указанием списка URL-адресов
    await crawler.run(['https://www.example.com'])

# Запуск асинхронной функции
asyncio.run(main())