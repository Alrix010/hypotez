Как использовать этот блок кода
=========================================================================================

Описание
-------------------------
Этот код реализует асинхронный генератор для взаимодействия с API AutonomousAI. Он позволяет отправлять сообщения и получать ответы от различных моделей, таких как `llama`, `qwen_coder`, `hermes`, `vision` и `summary`. Код поддерживает потоковую передачу данных (stream) и системные сообщения, а также предоставляет возможность использования прокси.

Шаги выполнения
-------------------------
1. **Подготовка**:
   - Определяются необходимые заголовки (`headers`) для HTTP-запроса.
   - Кодируется JSON-сообщение (`messages`) в Base64 для передачи в API.

2. **Отправка запроса**:
   - Создается асинхронная сессия (`ClientSession`) с заданными заголовками.
   - Формируется тело запроса (`data`) с закодированным сообщением, идентификатором потока (`threadId`), параметром потоковой передачи (`stream`) и идентификатором AI-агента (`aiAgent`).
   - Отправляется POST-запрос на указанный API endpoint с использованием `session.post`.

3. **Обработка ответа**:
   - Полученные чанки данных обрабатываются асинхронно.
   - Игнорируются сообщения `data: [DONE]`.
   - Из каждого чанка извлекается JSON, удаляется префикс `data: `.
   - Извлекается содержимое (`content`) из поля `delta` в `choices`, которое возвращается как часть генератора.
   - Если в чанке присутствует `finish_reason`, он также возвращается как `FinishReason`.

4. **Обработка ошибок**:
   - В случае ошибки декодирования JSON (`json.JSONDecodeError`), она игнорируется и происходит переход к следующему чанку.
   - Для проверки статуса ответа используется `raise_for_status`.

Пример использования
-------------------------

```python
from src.endpoints.gpt4free.g4f.Provider.not_working.AutonomousAI import AutonomousAI
import asyncio

async def main():
    model = "llama"
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]

    async for response in AutonomousAI.create_async_generator(model=model, messages=messages, stream=True):
        print(response, end="")

if __name__ == "__main__":
    asyncio.run(main())