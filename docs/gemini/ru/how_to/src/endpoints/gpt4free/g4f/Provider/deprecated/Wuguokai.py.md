### Как использовать этот блок кода
=========================================================================================

Описание
-------------------------
Этот код определяет класс `Wuguokai`, который является провайдером для работы с моделью GPT-3.5 Turbo через API `chat.wuguokai.xyz`. Он отправляет запросы к API и возвращает ответы, обрабатывая возможные ошибки и форматируя входные данные.

Шаги выполнения
-------------------------
1. **Определение класса `Wuguokai`**:
   - Класс наследуется от `AbstractProvider`.
   - Устанавливает URL `chat.wuguokai.xyz` в качестве базового адреса API.
   - Указывает, что провайдер поддерживает модель `gpt-3.5-turbo`.
   - Изначально устанавливает `working = False`, что может указывать на то, что провайдер неактивен до момента инициализации или проверки работоспособности.

2. **Метод `create_completion`**:
   - Принимает параметры: `model` (модель для использования), `messages` (список сообщений для отправки), `stream` (флаг для потоковой передачи данных) и `kwargs` (дополнительные аргументы, такие как прокси).
   - Формирует HTTP-заголовки, необходимые для запроса к API.
   - Подготавливает данные запроса в формате JSON, включая отформатированный промпт, опции и уникальный идентификатор пользователя.
   - Отправляет POST-запрос к API `https://ai-api20.wuguokai.xyz/api/chat-process` с использованием библиотеки `requests`.
   - Обрабатывает ответ от API:
     - Проверяет статус код ответа. Если код не равен 200, вызывает исключение.
     - Разделяет текст ответа по строке "> 若回答失败请重试或多刷新几次界面后重试".
     - Возвращает контент после разделения. Если разделение не выполнено, возвращает весь текст ответа.

Пример использования
-------------------------

```python
from src.endpoints.gpt4free.g4f.Provider.deprecated.Wuguokai import Wuguokai

# Пример использования провайдера Wuguokai для получения ответа от GPT-3.5 Turbo
provider = Wuguokai()
messages = [
    {"role": "user", "content": "Напиши короткий рассказ о космосе."}
]
try:
    response_generator = provider.create_completion(
        model="gpt-3.5-turbo",
        messages=messages,
        stream=False,
        proxy={}  # Укажите прокси, если необходимо
    )
    response = ""
    for chunk in response_generator:
        response += chunk
    print(response)
except Exception as e:
    print(f"Ошибка при выполнении запроса: {e}")