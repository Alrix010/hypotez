### **Как использовать блок кода `Ollama`**

=========================================================================================

Описание
-------------------------
Блок кода определяет класс `Ollama`, который является подклассом `OpenaiAPI` и предназначен для взаимодействия с локально установленным сервером Ollama. Этот класс позволяет получать список доступных моделей и создавать асинхронный генератор для взаимодействия с этими моделями.

Шаги выполнения
-------------------------
1. **Получение списка моделей**:
   - Функция `get_models` извлекает список моделей с сервера Ollama. Если список моделей еще не был получен, она выполняет HTTP-запрос к API Ollama.
   - Если `api_base` не указан, функция получает хост и порт из переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`. Если переменные окружения не установлены, используются значения по умолчанию "127.0.0.1" и "11434" соответственно.
   - Формируется URL для запроса списка моделей (`/api/tags`).
   - Отправляется GET-запрос к API Ollama и извлекается JSON-ответ, содержащий список моделей.
   - Извлекаются имена моделей из JSON-ответа и сохраняются в `cls.models`. Первая модель устанавливается как модель по умолчанию (`cls.default_model`).
   - Функция возвращает список доступных моделей.

2. **Создание асинхронного генератора**:
   - Функция `create_async_generator` создает асинхронный генератор для взаимодействия с моделями Ollama.
   - Если `api_base` не указан, функция получает хост и порт из переменных окружения `OLLAMA_HOST` и `OLLAMA_PORT`. Если переменные окружения не установлены, используются значения по умолчанию "localhost" и "11434" соответственно.
   - Формируется `api_base` для дальнейшего использования.
   - Вызывается метод `create_async_generator` родительского класса `OpenaiAPI` с переданными параметрами.

Пример использования
-------------------------

```python
import os
from src.endpoints.gpt4free.g4f.Provider.local.Ollama import Ollama
from src.typing import Messages

# Установка переменных окружения (опционально)
os.environ["OLLAMA_HOST"] = "localhost"
os.environ["OLLAMA_PORT"] = "11434"

# Получение списка моделей
models = Ollama.get_models()
print(f"Доступные модели: {models}")

# Пример сообщения для отправки модели
messages: Messages = [{"role": "user", "content": "Привет, как дела?"}]

# Создание асинхронного генератора
async_generator = Ollama.create_async_generator(model=Ollama.default_model, messages=messages)

# Дальнейшее взаимодействие с асинхронным генератором (пример)
# async for message in async_generator:
#     print(message)