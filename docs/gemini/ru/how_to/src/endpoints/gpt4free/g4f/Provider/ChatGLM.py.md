### **Инструкция по использованию блока кода**

=========================================================================================

Описание
-------------------------
Этот код реализует асинхронного провайдера `ChatGLM` для взаимодействия с API `chatglm.cn`. Он отправляет сообщения в API и генерирует текст ответа, обрабатывая чанки данных в режиме реального времени. Поддерживает потоковую передачу данных (streaming) и использует `aiohttp` для асинхронных запросов.

Шаги выполнения
-------------------------
1. **Подготовка заголовков**:
   - Генерируется `device_id` на основе `uuid`.
   - Формируются заголовки HTTP-запроса, включающие `device_id`, `User-Agent` и другие метаданные.
2. **Создание сессии `aiohttp`**:
   - Создается асинхронная сессия `aiohttp.ClientSession` с установленными заголовками.
3. **Формирование данных запроса**:
   - Создается словарь `data`, содержащий информацию о сообщении, идентификаторы ассистента и разговора, а также метаданные.
   - Сообщения форматируются в структуру, ожидаемую API `ChatGLM`.
4. **Отправка POST-запроса**:
   - Отправляется асинхронный `POST` запрос к `cls.api_endpoint` с использованием сессии `aiohttp`, передавая `data` в формате `json` и используя прокси, если он указан.
5. **Обработка потока ответов**:
   - Читаются чанки данных из ответа.
   - Декодируются чанки из `utf-8`.
   - Извлекаются данные `json` из чанков, начиная с `data: `.
   - Извлекается контент из `json_data['parts'][0]['content'][0]['text']`.
   - Вычисляется разница между полученным текстом и уже переданным текстом (`text[yield_text:]`).
   - Если есть новый текст, он передается через `yield`.
   - Если статус ответа `'finish'`, передается `FinishReason("stop")`.
6. **Обработка ошибок JSON**:
   - В случае ошибки декодирования JSON (`json.JSONDecodeError`) обработка чанка пропускается.

Пример использования
-------------------------

```python
import asyncio
from src.endpoints.gpt4free.g4f.Provider.ChatGLM import ChatGLM
from src.endpoints.gpt4free.g4f.typing import Message

async def main():
    messages: list[Message] = [
        {"role": "user", "content": "Hello, how are you?"}
    ]
    
    async for response in ChatGLM.create_async_generator(
        model="glm-4",
        messages=messages
    ):
        print(response, end="")

if __name__ == "__main__":
    asyncio.run(main())