### Анализ кода модуля `src/webdriver/crawlee_python/readme.ru.md`

## Обзор

Этот модуль представляет собой документацию на русском языке для модуля, реализующего веб-скрейпинг с использованием Crawlee и Playwright.

## Подробней

Файл `src/webdriver/crawlee_python/readme.ru.md` предоставляет описание модуля `src/webdriver/crawlee_python/crawlee_python.py`, который использует библиотеку Crawlee для управления Playwright и автоматизации задач веб-скрейпинга. Документ содержит информацию об установке, настройке и использовании модуля.

## Ключевые особенности

-   **Централизованная конфигурация**: Параметры для скрейпинга управляются через файл `crawlee_python.json`.
-   **Поддержка пользовательских опций**: Возможность передачи дополнительных опций при инициализации.
-   **Улучшенное логирование и обработка ошибок**: Предоставляет детальные логи для инициализации, проблем с конфигурацией и ошибок WebDriver.
-   **Поддержка прокси**: Позволяет настроить прокси-сервер для обхода ограничений.
-   **Гибкие настройки браузера**: Позволяет настраивать размер окна просмотра, User-Agent и другие параметры браузера.

## Требования

Для использования этого модуля необходимо установить следующие зависимости:

-   Python 3.x
-   Playwright
-   Crawlee

Команда для установки зависимостей:

```bash
pip install playwright crawlee
```

Также необходимо убедиться, что Playwright установлен и настроен для работы с браузером. Установите браузеры, используя команду:

```bash
playwright install
```

## Конфигурация

Конфигурация для Crawlee Python хранится в файле `crawlee_python.json`. Ниже приведён пример структуры конфигурационного файла и его описание:

### Пример конфигурации (`crawlee_python.json`)

```json
{
  "max_requests": 10,
  "headless": true,
  "browser_type": "chromium",
  "options": [
    "--disable-dev-shm-usage",
    "--no-sandbox",
    "--disable-gpu"
  ],
  "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
  "proxy": {
    "enabled": false,
    "server": "http://proxy.example.com:8080",
    "username": "user",
    "password": "password"
  },
  "viewport": {
    "width": 1280,
    "height": 720
  },
  "timeout": 30000,
  "ignore_https_errors": false
}
```

### Описание полей конфигурации

#### 1. `max_requests`

Максимальное количество запросов, которые будут выполнены во время обхода. По умолчанию `10`.

#### 2. `headless`

Булевое значение, указывающее, следует ли запускать браузер в безголовом режиме. По умолчанию `true`.

#### 3. `browser_type`

Тип используемого браузера. Возможные значения:

-   `chromium` (по умолчанию)
-   `firefox`
-   `webkit`

#### 4. `options`

Список аргументов командной строки, передаваемых в браузер. Примеры:

-   `--disable-dev-shm-usage`: Отключает использование `/dev/shm` в Docker-контейнерах.
-   `--no-sandbox`: Отключает режим песочницы.
-   `--disable-gpu`: Отключает аппаратное ускорение GPU.

#### 5. `user_agent`

Строка user-agent, используемая в запросах браузера.

#### 6. `proxy`

Настройки прокси-сервера:

-   `enabled`: Булевое значение, указывающее, следует ли использовать прокси.
-   `server`: Адрес прокси-сервера.
-   `username`: Имя пользователя для аутентификации на прокси-сервере.
-   `password`: Пароль для аутентификации на прокси-сервере.

#### 7. `viewport`

Размеры окна браузера:

-   `width`: Ширина окна.
-   `height`: Высота окна.

#### 8. `timeout`

Максимальное время ожидания для выполнения операций (в миллисекундах). По умолчанию `30000` (30 секунд).

#### 9. `ignore_https_errors`

Булевое значение, указывающее, следует ли игнорировать ошибки HTTPS. По умолчанию `false`.

## Использование

Чтобы использовать `CrawleePython` в своём проекте, просто импортируйте его и инициализируйте:

```python
from src.webdriver.crawlee_python import CrawleePython
import asyncio

# Инициализация CrawleePython с пользовательскими опциями
async def main():
    crawler = CrawleePython(max_requests=10, headless=True, browser_type='chromium', options=["--headless"])
    await crawler.run(['https://www.example.com'])

asyncio.run(main())
```

Класс `CrawleePython` автоматически загружает настройки из файла `crawlee_python.json` и использует их для конфигурации WebDriver. Также можно указать пользовательский user-agent и передать дополнительные опции при инициализации WebDriver.

## Логирование и отладка

Класс WebDriver использует `logger` из `src.logger` для логирования ошибок, предупреждений и общей информации. Все проблемы, возникающие при инициализации, конфигурации или выполнении, будут записываться в логи для удобства отладки.

### Примеры логов

-   **Ошибка при инициализации WebDriver**: `Ошибка при инициализации Crawlee Python: <детали ошибки>`
-   **Проблемы с конфигурацией**: `Ошибка в файле crawlee_python.json: <детали проблемы>`

## Лицензия

Этот проект лицензирован на условиях MIT License — см. файл [LICENSE](../../LICENSE) для подробностей.