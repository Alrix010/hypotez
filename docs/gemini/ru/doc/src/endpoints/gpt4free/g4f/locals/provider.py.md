# Модуль для работы с локальными провайдерами моделей GPT4All
## Обзор

Модуль `provider.py` предназначен для работы с локальными моделями GPT4All. Он предоставляет класс `LocalProvider`, который позволяет создавать завершения на основе локально установленных моделей. Модуль отвечает за поиск моделей, их загрузку и генерацию ответов на основе предоставленных сообщений.

## Подробнее

Модуль содержит функции для поиска директории модели, загрузки моделей и класс `LocalProvider` для создания завершений. Он использует библиотеку `gpt4all` для взаимодействия с локальными моделями и предоставляет возможность потоковой генерации ответов.
В проекте `hypotez` этот модуль позволяет использовать локальные модели GPT4All для генерации текста, что может быть полезно в ситуациях, когда нет доступа к интернету или требуется более быстрый ответ.

## Функции

### `find_model_dir`

```python
def find_model_dir(model_file: str) -> str:
    """Определяет путь к каталогу, в котором хранится файл модели.

    Args:
        model_file (str): Имя файла модели.

    Returns:
        str: Путь к каталогу, содержащему файл модели.
    """
```

**Назначение**:
Функция `find_model_dir` предназначена для поиска директории, в которой находится указанный файл модели. Она ищет модель в нескольких местах: в директории с текущим скриптом, в директории `models` на уровень выше, а также в текущей рабочей директории.

**Параметры**:
- `model_file` (str): Имя файла модели, который нужно найти.

**Возвращает**:
- `str`: Путь к директории, содержащей файл модели. Если файл не найден ни в одной из проверенных директорий, возвращается путь к директории `models` на уровень выше.

**Как работает функция**:
1.  Определяет абсолютный путь к директории, в которой находится текущий скрипт.
2.  Определяет путь к директории `models` на уровень выше.
3.  Проверяет, существует ли файл модели в новой директории `models`. Если да, возвращает путь к этой директории.
4.  Определяет путь к директории `models` в той же директории, что и скрипт.
5.  Проверяет, существует ли файл модели в старой директории `models`. Если да, возвращает путь к этой директории.
6.  Устанавливает текущую рабочую директорию.
7.  Выполняет поиск файла модели во всех поддиректориях текущей рабочей директории. Если файл найден, возвращает путь к директории, в которой он находится.
8.  Если файл не найден ни в одной из проверенных директорий, возвращает путь к новой директории `models`.

**Примеры**:

```python
model_file = "ggml-model.bin"
model_dir = find_model_dir(model_file)
print(model_dir)
```

## Классы

### `LocalProvider`

**Описание**:
Класс `LocalProvider` предоставляет функциональность для создания завершений с использованием локальных моделей GPT4All. Он загружает модели и генерирует ответы на основе предоставленных сообщений.

**Атрибуты**:
- Отсутствуют атрибуты, специфичные для класса.

**Методы**:
- `create_completion`: Создает завершение на основе локальной модели GPT4All.

### Методы класса

#### `create_completion`

```python
@staticmethod
def create_completion(model: str, messages: Messages, stream: bool = False, **kwargs):
    """Создает завершение на основе локальной модели GPT4All.

    Args:
        model (str): Имя модели.
        messages (Messages): Список сообщений для генерации ответа.
        stream (bool): Флаг, указывающий, следует ли использовать потоковую генерацию.
        **kwargs: Дополнительные аргументы.

    Returns:
        Generator[str, None, None] | str: Генератор токенов или строка с завершением.

    Raises:
        ValueError: Если модель не найдена или не реализована.
        ValueError: Если файл модели не найден.
    """
```

**Назначение**:
Метод `create_completion` отвечает за создание завершений на основе локальной модели GPT4All. Он принимает имя модели, список сообщений и флаг потоковой генерации.

**Параметры**:
- `model` (str): Имя модели, которую нужно использовать.
- `messages` (Messages): Список сообщений для генерации ответа.
- `stream` (bool): Флаг, указывающий, следует ли использовать потоковую генерацию. По умолчанию `False`.
- `**kwargs`: Дополнительные аргументы.

**Возвращает**:
- `Generator[str, None, None] | str`: Генератор токенов, если `stream` равен `True`, или строка с завершением, если `stream` равен `False`.

**Как работает функция**:
1.  Проверяет, инициализирован ли список моделей `MODEL_LIST`. Если нет, вызывает функцию `get_models()` для его инициализации.
2.  Проверяет, существует ли указанная модель в списке `MODEL_LIST`. Если нет, вызывает исключение `ValueError`.
3.  Извлекает информацию о модели из списка `MODEL_LIST`.
4.  Определяет путь к файлу модели с помощью функции `find_model_dir`.
5.  Проверяет, существует ли файл модели. Если нет, предлагает пользователю загрузить модель.
6.  Инициализирует модель GPT4All с указанными параметрами.
7.  Формирует системное сообщение из сообщений с ролью "system".
8.  Определяет шаблон промпта и формирует строку беседы из сообщений с ролями, отличными от "system".
9.  Определяет функцию `should_not_stop`, которая определяет, следует ли остановить генерацию.
10. Использует `model.chat_session` для управления состоянием чата и генерации ответа.
11. Если `stream` равен `True`, генерирует ответ потоково, возвращая генератор токенов.
12. Если `stream` равен `False`, генерирует полный ответ и возвращает его в виде строки.

**Примеры**:

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is the capital of France?"}
]
model_name = "ggml-model.bin" # Замените на имя вашей локальной модели
# response = LocalProvider.create_completion(model=model_name, messages=messages)
# print(response)
```

## Параметры класса

-   Нет специфических параметров класса.