# Модуль `src.llm.gemini.gemini`

## Обзор

Модуль `src.llm.gemini.gemini` предоставляет интеграцию с Google Generative AI, используя библиотеку `google.generativeai`. Он включает в себя класс `GoogleGenerativeAi`, который упрощает взаимодействие с моделями Gemini для генерации текста, обработки изображений и загрузки файлов. Модуль поддерживает управление историей чатов, обработку ошибок и повторные попытки при сбоях API.

## Подробнее

Модуль предназначен для интеграции в проекты, требующие взаимодействия с моделями Google Gemini. Он обеспечивает удобный интерфейс для отправки запросов к моделям, обработки ответов и управления историей диалогов. В модуле реализована обработка различных ошибок, которые могут возникнуть при взаимодействии с API, а также механизмы повторных попыток для повышения устойчивости к сбоям. Расположение файла в проекте указывает на то, что он является частью подсистемы, отвечающей за взаимодействие с большими языковыми моделями (LLM).

## Классы

### `GoogleGenerativeAi`

**Описание**: Класс для взаимодействия с моделями Google Generative AI.

**Атрибуты**:
- `ENDPOINT` (Path): Путь к директории `gemini` внутри `src/llm/gemini`.
- `config` (SimpleNamespace): Конфигурация, загруженная из файла `gemini.json`.
- `api_key` (str): Ключ API для доступа к Google Generative AI.
- `system_instruction` (str): Системная инструкция для модели.
- `model_name` (str): Имя используемой модели Gemini.
- `model` (genai.GenerativeModel): Инициализированный клиент модели `genai.GenerativeModel`.
- `timestamp` (str): Текущая временная метка для именования файлов истории.
- `_chat` (Any): Активный сеанс чата с моделью.
- `chat_history` (List[Dict]): История текущего диалога в памяти.
- `chat_name` (str): Имя текущего чата для сохранения истории.
- `history_dir` (Path): Директория для сохранения истории чатов.
- `history_json_file` (Path): Путь к JSON файлу с историей текущего чата.
- `dialogue_txt_path` (Path): Путь к текстовому файлу с диалогом (не используется активно).
- `history_txt_file` (Path): Путь к текстовому файлу с историей (не используется активно).

**Методы**:
- `__init__`: Инициализирует экземпляр класса `GoogleGenerativeAi`.
- `_start_chat`: Запускает новый сеанс чата с моделью.
- `_save_chat_history`: Асинхронно сохраняет текущую историю чата в JSON файл.
- `_load_chat_history`: Асинхронно загружает историю чата из JSON файла.
- `clear_history`: Очищает историю чата в памяти и удаляет связанный JSON файл истории.
- `chat`: Обрабатывает чат-запрос пользователя, управляет историей и возвращает ответ модели.
- `ask`: Синхронно отправляет текстовый запрос модели и возвращает ответ.
- `ask_async`: Асинхронно отправляет текстовый запрос модели и возвращает ответ.
- `describe_image`: Отправляет изображение (и опциональный промпт) в модель Gemini Pro Vision и возвращает его текстовое описание.
- `upload_file`: Асинхронно загружает файл в Google AI File API.

### `__init__`
```python
def __init__(
        self,
        api_key: str,
        model_name: str,
        generation_config: Optional[Dict] = {'response_mime_type': 'text/plain'},
        system_instruction: Optional[str] = None,
    ):
```
**Назначение**: Инициализирует экземпляр класса `GoogleGenerativeAi`.

**Параметры**:
- `api_key` (str): Ключ API для Google Generative AI.
- `model_name` (str): Имя модели Gemini для использования (например, `'gemini-pro'`).
- `generation_config` (Dict, optional): Конфигурация генерации. По умолчанию `{'response_mime_type': 'text/plain'}`.
- `system_instruction` (Optional[str], optional): Системная инструкция для модели. По умолчанию `None`.

**Как работает функция**:
- Присваивает значения атрибутам экземпляра класса на основе переданных аргументов.
- Инициализирует клиент модели `genai.GenerativeModel` с использованием ключа API, имени модели и конфигурации генерации.
- Запускает новый сеанс чата с помощью метода `_start_chat`.
- Логирует информацию об успешной инициализации модели.
- Обрабатывает исключения, которые могут возникнуть при инициализации модели, и повторно вызывает их для прерывания процесса.

**Примеры**:
```python
llm = GoogleGenerativeAi(
    api_key='YOUR_API_KEY',
    model_name='gemini-pro',
    system_instruction='Ты - полезный ассистент.'
)
```

### `_start_chat`
```python
def _start_chat(self,system_instruction: Optional[str] = '') -> Any:
```
**Назначение**: Запускает новый сеанс чата с моделью.

**Параметры**:
- `system_instruction` (Optional[str], optional): Системная инструкция для модели. По умолчанию `''`.

**Возвращает**:
- `Any`: Объект сеанса чата (`genai.ChatSession`).

**Как работает функция**:
- Проверяет наличие системной инструкции (`self.system_instruction`).
- Если системная инструкция присутствует, запускает чат с использованием этой инструкции.
- Если системная инструкция отсутствует, запускает чат без системной инструкции.

**Примеры**:
```python
chat_session = llm._start_chat()
```

### `_save_chat_history`
```python
async def _save_chat_history(self) -> bool:
```
**Назначение**: Асинхронно сохраняет текущую историю чата в JSON файл.

**Возвращает**:
- `bool`: `True` в случае успешного сохранения, `False` при ошибке.

**Как работает функция**:
- Формирует имя файла для сохранения истории чата на основе `chat_name` и `timestamp`.
- Создает директорию для сохранения истории чата, если она не существует.
- Сохраняет историю чата в JSON файл с использованием функции `j_dumps`.
- Логирует информацию об успешном сохранении или ошибке.

**Примеры**:
```python
await llm._save_chat_history()
```

### `_load_chat_history`
```python
async def _load_chat_history(self, chat_data_folder: Optional[str | Path]) -> None:
```
**Назначение**: Асинхронно загружает историю чата из JSON файла.

**Параметры**:
- `chat_data_folder` (Optional[str | Path]): Путь к папке с файлом `'history.json'`.

**Как работает функция**:
- Определяет целевой файл для загрузки истории чата. Если `chat_data_folder` указан, используется файл `'history.json'` в этой папке, иначе используется `self.history_json_file`.
- Загружает историю чата из JSON файла с использованием функции `j_loads`.
- Если история чата успешно загружена, перезапускает чат с загруженной историей.
- Логирует информацию об успешной загрузке или ошибке.

**Примеры**:
```python
await llm._load_chat_history(chat_data_folder='path/to/chat')
```

### `clear_history`
```python
def clear_history(self) -> None:
```
**Назначение**: Очищает историю чата в памяти и удаляет связанный JSON файл истории.

**Как работает функция**:
- Очищает историю чата в памяти, присваивая `self.chat_history` пустой список.
- Удаляет JSON файл истории, если он существует.
- Логирует информацию об успешной очистке или ошибке.

**Примеры**:
```python
llm.clear_history()
```

### `chat`
```python
async def chat(self, q: str, chat_name: str, flag: Optional[str] = 'save_chat') -> Optional[str]:
```
**Назначение**: Обрабатывает чат-запрос пользователя, управляет историей и возвращает ответ модели.

**Параметры**:
- `q` (str): Вопрос пользователя.
- `chat_name` (str): Имя чата для сохранения/загрузки истории.
- `flag` (str, optional): Режим управления историей. По умолчанию `'save_chat'`.

**Возвращает**:
- `Optional[str]`: Текстовый ответ модели или `None` в случае ошибки.

**Как работает функция**:
- Устанавливает имя чата для сохранения истории.
- Отправляет запрос модели с использованием метода `_chat.send_message_async`.
- Извлекает метаданные об использовании токенов из ответа модели.
- Добавляет запрос и ответ в историю чата.
- Сохраняет обновленную историю чата.
- Логирует информацию об ошибках и предупреждениях.

**Примеры**:
```python
response = await llm.chat(
    q='Какой сегодня день?',
    chat_name='test_chat'
)
```

### `ask`
```python
def ask(self, q: str, attempts: int = 15, save_dialogue: bool = False, clean_response: bool = True) -> Optional[str]:
```
**Назначение**: Синхронно отправляет текстовый запрос модели и возвращает ответ.

**Параметры**:
- `q` (str): Текстовый запрос к модели.
- `attempts` (int): Количество попыток отправки запроса. По умолчанию `15`.
- `save_dialogue` (bool): Флаг сохранения диалога (вопрос/ответ) в файл. По умолчанию `False`.
- `clean_response` (bool): Флаг очистки ответа от разметки кода. По умолчанию `True`.

**Возвращает**:
- `Optional[str]`: Текстовый ответ модели или `None` в случае неудачи после всех попыток.

**Как работает функция**:
- Отправляет запрос модели с использованием метода `generate_content`.
- Повторяет запрос при возникновении ошибок.
- Сохраняет диалог в файл, если установлен флаг `save_dialogue`.
- Очищает ответ от разметки кода, если установлен флаг `clean_response`.
- Логирует информацию об ошибках и предупреждениях.

**Примеры**:
```python
response = llm.ask(
    q='Расскажи мне о Python.',
    attempts=5,
    clean_response=True
)
```

### `ask_async`
```python
async def ask_async(self, q: str, attempts: int = 15, save_dialogue: bool = False, clean_response: bool = True) -> Optional[str]:
```
**Назначение**: Асинхронно отправляет текстовый запрос модели и возвращает ответ.

**Параметры**:
- `q` (str): Текстовый запрос к модели.
- `attempts` (int): Количество попыток отправки запроса. По умолчанию `15`.
- `save_dialogue` (bool): Флаг сохранения диалога (вопрос/ответ) в файл. По умолчанию `False`.
- `clean_response` (bool): Флаг очистки ответа от разметки кода. По умолчанию `True`.

**Возвращает**:
- `Optional[str]`: Текстовый ответ модели или `None` в случае неудачи после всех попыток.

**Как работает функция**:
- Отправляет запрос модели с использованием метода `generate_content_async` в отдельном потоке.
- Повторяет запрос при возникновении ошибок.
- Сохраняет диалог в файл, если установлен флаг `save_dialogue`.
- Очищает ответ от разметки кода, если установлен флаг `clean_response`.
- Логирует информацию об ошибках и предупреждениях.

**Примеры**:
```python
response = await llm.ask_async(
    q='Расскажи мне о Python.',
    attempts=5,
    clean_response=True
)
```

### `describe_image`
```python
def describe_image(
        self, image: Path | bytes, mime_type: Optional[str] = 'image/jpeg', prompt: Optional[str] = ''
    ) -> Optional[str]:
```
**Назначение**: Отправляет изображение (и опциональный промпт) в модель Gemini Pro Vision и возвращает его текстовое описание.

**Параметры**:
- `image` (Path | bytes): Путь к файлу изображения или байты изображения.
- `mime_type` (Optional[str]): MIME-тип изображения. По умолчанию `'image/jpeg'`.
- `prompt` (Optional[str]): Текстовый промпт для модели вместе с изображением. По умолчанию `''`.

**Возвращает**:
- `Optional[str]`: Текстовое описание изображения от модели или `None` при ошибке.

**Как работает функция**:
- Подготавливает данные изображения, извлекая байты из файла или используя переданные байты.
- Формирует контент для запроса, включая промпт и данные изображения.
- Отправляет запрос модели с использованием метода `generate_content`.
- Извлекает текст из ответа модели.
- Логирует информацию об ошибках и предупреждениях.

**Примеры**:
```python
response = llm.describe_image(
    image='path/to/image.jpg',
    prompt='Опиши это изображение.'
)
```

### `upload_file`
```python
async def upload_file(
        self, file: str | Path | IOBase, file_name: Optional[str] = None
    ) -> Optional[Any]:
```
**Назначение**: Асинхронно загружает файл в Google AI File API.

**Параметры**:
- `file` (str | Path | IOBase): Путь к файлу или файловый объект.
- `file_name` (Optional[str]): Имя файла для отображения в API. Если `None`, используется имя из пути файла.

**Возвращает**:
- `Optional[Any]`: Объект `File` от API в случае успеха, иначе `None`.

**Как работает функция**:
- Определяет путь и имя файла на основе переданных аргументов.
- Загружает файл в Google AI File API с использованием метода `genai.upload_file_async`.
- Логирует информацию об успешной загрузке или ошибке.

**Примеры**:
```python
response = await llm.upload_file(
    file='path/to/file.txt',
    file_name='example.txt'
)
```

## Функции

### `main`
```python
async def main():
```

**Назначение**: Основная асинхронная функция для демонстрации работы класса.

**Как работает функция**:
- Проверяет наличие ключа API Gemini.
- Инициализирует экземпляр класса `GoogleGenerativeAi`.
- Выполняет примеры:
  - Описания изображения с использованием `describe_image`.
  - Загрузки файла с использованием `upload_file`.
  - Чата с использованием `chat`.
- Логирует информацию о процессе выполнения и результатах.

**Примеры**:
```python
asyncio.run(main())