# Модуль для запуска задач с использованием LLM через LangChain и кастомного Agent.

## Обзор

Модуль предоставляет функциональность для конфигурирования моделей (Gemini, OpenAI) через словарь, установки API ключей в переменные окружения, запуска задачи на всех активных моделях и сбора результатов от каждой активной модели.

## Подробнее

Модуль предназначен для упрощения взаимодействия с различными LLM (Large Language Models) через LangChain и кастомного Agent `browser_use`. Он позволяет динамически конфигурировать модели, устанавливать API ключи и выполнять задачи, собирая результаты от каждой модели. Модуль использует переменные окружения для хранения API ключей, что повышает безопасность и гибкость конфигурации.

## Классы

### `Config`

**Описание**: Класс для хранения статической конфигурации приложения.

**Атрибуты**:
- `ENDPOINT` (Path): Путь к директории, где расположены конфигурационные файлы.
- `config` (Dict[str, Any]): Словарь с общей конфигурацией приложения, загруженный из `use_ai.json`.
- `models_to_use` (Dict[str, Dict[str, str]]): Словарь с конфигурациями используемых моделей.

**Принцип работы**:
Класс `Config` предназначен для хранения статических параметров конфигурации приложения, таких как пути к файлам конфигурации, параметры моделей и API ключи. API ключи устанавливаются в переменные окружения при инициализации класса из `gs.credentials`.

**Методы**:
- Отсутствуют явно определенные методы, но класс выполняет установку API ключей в переменные окружения при инициализации.

## Функции

### `_get_active_model_configs`

**Назначение**: Фильтрует словарь конфигураций моделей, оставляя только активные.

**Параметры**:
- `models_config` (Dict[str, Dict[str, str]]): Словарь с конфигурациями всех моделей.

**Возвращает**:
- Словарь, содержащий только конфигурации моделей со статусом `'active'`.

**Как работает функция**:
Функция `_get_active_model_configs` принимает словарь конфигураций моделей и возвращает новый словарь, содержащий только те модели, у которых статус установлен в `'active'`. Если формат входного словаря некорректен, функция логирует ошибку и возвращает пустой словарь.

**Примеры**:

```python
models_config = {
    'gemini': {'status': 'active', 'model_name': 'gemini-1.0-pro'},
    'openai': {'status': 'inactive', 'model_name': 'gpt-3.5-turbo'}
}
active_models = _get_active_model_configs(models_config)
print(active_models)  # {'gemini': {'status': 'active', 'model_name': 'gemini-1.0-pro'}}
```

### `_initialize_llm`

**Назначение**: Инициализирует экземпляр LLM на основе провайдера и конфигурации.

**Параметры**:
- `provider_key` (str): Идентификатор провайдера (например, `'gemini'`, `'openai'`).
- `model_details` (Dict[str, str]): Словарь с конфигурацией модели (должен содержать `'model_name'`).

**Возвращает**:
- Инициализированный экземпляр `BaseChatModel` или `None` в случае ошибки.

**Как работает функция**:
Функция `_initialize_llm` принимает идентификатор провайдера и конфигурацию модели, после чего пытается инициализировать соответствующий экземпляр LLM (например, `ChatGoogleGenerativeAI` для Gemini или `ChatOpenAI` для OpenAI). API ключ читается из переменных окружения. В случае успеха возвращается инициализированный экземпляр модели, иначе - `None`.

**Примеры**:

```python
model_details = {'model_name': 'gemini-1.0-pro'}
llm_instance = _initialize_llm('gemini', model_details)
print(llm_instance)  # <langchain_google_genai.chat_models.ChatGoogleGenerativeAI object at 0x...>
```

### `_run_agent_task`

**Назначение**: Инициализирует и запускает Agent с предоставленной моделью и задачей.

**Параметры**:
- `llm` (BaseChatModel): Инициализированный экземпляр LLM.
- `task` (str): Текст задачи для выполнения Agent'ом.
- `provider_key` (str): Имя провайдера для логирования.

**Возвращает**:
- `Optional[str]`: Результат выполнения задачи Agent'ом в виде строки, или `None` в случае ошибки или если Agent вернул `None`.

**Как работает функция**:
Функция `_run_agent_task` принимает экземпляр LLM, задачу и имя провайдера, после чего инициализирует Agent с переданными параметрами и запускает выполнение задачи. Результат выполнения задачи возвращается в виде строки. В случае ошибки или если Agent возвращает `None`, функция возвращает `None`.

**Примеры**:

```python
async def example():
    model_details = {'model_name': 'gemini-1.0-pro'}
    llm_instance = _initialize_llm('gemini', model_details)
    if llm_instance:
        task = "Напиши краткое описание товара 'беспроводные наушники'."
        result = await _run_agent_task(llm_instance, task, 'gemini')
        print(result)  # Краткое описание товара
asyncio.run(example())
```

### `run_task_on_active_models`

**Назначение**: Выполняет задачу на ВСЕХ активных моделях из конфигурации асинхронно.

**Параметры**:
- `task` (str): Текст задачи для передачи агенту.
- `app_config` (Config, optional): Экземпляр конфигурации приложения. По умолчанию `Config()`.

**Возвращает**:
- `Optional[Dict[str, Optional[str]]]`: Словарь, где ключ - идентификатор модели (например, `'gemini'` или `'openai'`), а значение - результат (str) или `None` (при ошибке). Возвращает `None`, если ни одна модель в конфигурации не была активной или если произошла критическая ошибка при подготовке задач.

**Как работает функция**:
Функция `run_task_on_active_models` выполняет задачу на всех активных моделях, определенных в конфигурации. Она получает список активных моделей, инициализирует их и запускает выполнение задачи параллельно для каждой модели с помощью `asyncio.gather`. Результаты выполнения собираются в словарь, где ключом является идентификатор модели, а значением - результат выполнения или `None` в случае ошибки.

**Примеры**:

```python
async def example():
    task = "Напиши краткое описание товара 'беспроводные наушники'."
    results = await run_task_on_active_models(task)
    print(results)  # {'gemini': 'Краткое описание товара', 'openai': 'Краткое описание товара'}
asyncio.run(example())
```

### `main`

**Назначение**: Основная асинхронная точка входа для запуска обработки страниц.

**Параметры**:
- Отсутствуют.

**Возвращает**:
- `None`

**Как работает функция**:
Функция `main` является основной точкой входа в приложение. Она создает экземпляр конфигурации, итерируется по списку URL, формирует задачу для каждой страницы и запускает ее выполнение на всех активных моделях с помощью `run_task_on_active_models`. Результаты выполнения выводятся в консоль.

**Примеры**:

```python
async def example():
    await main()
asyncio.run(example())
```

## Внутренние функции

В данном модуле нет внутренних функций.