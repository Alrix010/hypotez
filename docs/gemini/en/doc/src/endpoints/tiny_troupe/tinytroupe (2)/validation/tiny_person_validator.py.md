# TinyPersonValidator Module

## Overview

This module defines the `TinyPersonValidator` class, which is responsible for validating `TinyPerson` instances using OpenAI's LLM. The validation process involves sending a series of questions to the `TinyPerson` and evaluating its responses against expectations.

## Details

The `TinyPersonValidator` class uses a combination of OpenAI's LLM, a mustache template for generating prompts, and the `TinyPerson` class to perform validation. The validation process involves:

1. **Generating prompts**: The `validate_person` method generates a prompt using a mustache template that incorporates user-defined expectations and the `TinyPerson`'s persona.
2. **Sending prompts to the LLM**: The generated prompt is sent to OpenAI's LLM, which then asks questions based on the prompt.
3. **Interviewing the TinyPerson**: The `TinyPerson` instance is then interviewed using the LLM's generated questions. The `TinyPerson` responds to the questions, and its answers are recorded.
4. **Evaluating responses**: The LLM receives the `TinyPerson`'s responses and evaluates them based on the expectations specified in the prompt.
5. **Calculating confidence score**: The LLM calculates a confidence score representing how well the `TinyPerson`'s responses align with the expectations.
6. **Returning the result**: The `validate_person` method returns the confidence score and a justification for the score.

## Classes

### `TinyPersonValidator`

**Description**: This class validates `TinyPerson` instances using OpenAI's LLM.

**Methods**:

- `validate_person(person, expectations=None, include_agent_spec=True, max_content_length=default_max_content_display_length) -> tuple[float, str]`

## Class Methods

### `validate_person(person, expectations=None, include_agent_spec=True, max_content_length=default_max_content_display_length) -> tuple[float, str]`

**Purpose**: Validates a `TinyPerson` instance using OpenAI's LLM.

**Parameters**:

- `person (TinyPerson)`: The `TinyPerson` instance to be validated.
- `expectations (str, optional)`: The expectations to be used in the validation process. Defaults to `None`.
- `include_agent_spec (bool, optional)`: Whether to include the agent specification in the prompt. Defaults to `True`.
- `max_content_length (int, optional)`: The maximum length of the content to be displayed when rendering the conversation. Defaults to `default_max_content_display_length`.

**Returns**:

- `float`: The confidence score of the validation process (0.0 to 1.0), or `None` if the validation process fails.
- `str`: The justification for the validation score, or `None` if the validation process fails.

**Raises Exceptions**:

- `None`: No specific exceptions are raised.

**How the Function Works**:

1. **Initialize messages**: The function initializes a list called `current_messages` to store the messages exchanged between the LLM and the `TinyPerson`.
2. **Generate prompt**: The function generates a prompt using a mustache template (`check_person.mustache`) that incorporates expectations and the `TinyPerson`'s persona. 
3. **Send initial messages**: The function sends initial messages to the LLM, including the system prompt (expectations) and the user prompt, which asks the LLM to create questions and interview the `TinyPerson`.
4. **Iterate through conversation**: The function enters a loop that continues until a specific termination mark is found in the LLM's response. Inside the loop:
    - **Read LLM questions**: The function reads the LLM's questions from the `message["content"]` variable.
    - **Interview the TinyPerson**: The function asks the `TinyPerson` the questions generated by the LLM, and stores the responses.
    - **Send responses to the LLM**: The function sends the `TinyPerson`'s responses back to the LLM.
    - **Read next message**: The function reads the next message from the LLM.
5. **Process final message**: Once the termination mark is found, the function extracts a JSON object from the message containing the validation score and justification.
6. **Return results**: The function returns the validation score and justification.

**Examples**:

```python
from tinytroupe.agent import TinyPerson
from tinytroupe.validation.tiny_person_validator import TinyPersonValidator

# Example TinyPerson instance
person = TinyPerson(name="Alice", minibio="Alice is a friendly and helpful person who enjoys learning about new things.")

# Example validation with expectations
expectations = "The person being interviewed is a kind and helpful person. They enjoy learning new things and are always eager to help others."
score, justification = TinyPersonValidator.validate_person(person, expectations=expectations)

print(f"Validation score: {score:.2f}")
print(f"Justification: {justification}")

# Example validation without expectations
score, justification = TinyPersonValidator.validate_person(person)

print(f"Validation score: {score:.2f}")
print(f"Justification: {justification}")
```

## Parameter Details

- `person (TinyPerson)`: The `TinyPerson` instance to be validated. This parameter is an instance of the `TinyPerson` class, which represents a conversational agent.
- `expectations (str, optional)`: The expectations to be used in the validation process. This parameter defines the criteria for evaluating the `TinyPerson`'s responses.
- `include_agent_spec (bool, optional)`: Whether to include the agent specification in the prompt. If `True`, the `TinyPerson`'s persona is included in the prompt, providing additional context for the LLM.
- `max_content_length (int, optional)`: The maximum length of the content to be displayed when rendering the conversation. This parameter controls the amount of text displayed during the interview process.

## Inner Functions

- **None**: There are no inner functions defined in the `validate_person` method.

```python
                import os
import json
import chevron
import logging

from tinytroupe import openai_utils
from tinytroupe.agent import TinyPerson
from tinytroupe import config
import tinytroupe.utils as utils

default_max_content_display_length = config["OpenAI"].getint("MAX_CONTENT_DISPLAY_LENGTH", 1024)


class TinyPersonValidator:

    @staticmethod
    def validate_person(person, expectations=None, include_agent_spec=True, max_content_length=default_max_content_display_length) -> tuple[float, str]:
        """
        Проверяет экземпляр TinyPerson с использованием LLM OpenAI.

        Этот метод отправляет серию вопросов экземпляру TinyPerson, чтобы проверить его ответы с использованием LLM OpenAI.
        Метод возвращает значение с плавающей запятой, представляющее степень уверенности процесса проверки.
        Если процесс проверки завершается неудачей, метод возвращает None.

        Args:
            person (TinyPerson): Экземпляр TinyPerson, который нужно проверить.
            expectations (str, optional): Ожидания, которые будут использоваться в процессе проверки. По умолчанию None.
            include_agent_spec (bool, optional): Включать ли спецификацию агента в запрос. По умолчанию True.
            max_content_length (int, optional): Максимальная длина контента, который будет отображаться при отображении разговора.

        Returns:
            float: Степень уверенности процесса проверки (от 0.0 до 1.0) или None, если процесс проверки завершается неудачей.
            str: Обоснование для оценки проверки или None, если процесс проверки завершается неудачей.
        """
        # Инициализация текущих сообщений
        current_messages = []

        # Генерация запроса для проверки персоны
        check_person_prompt_template_path = os.path.join(os.path.dirname(__file__), 'prompts/check_person.mustache')
        with open(check_person_prompt_template_path, 'r') as f:
            check_agent_prompt_template = f.read()

        system_prompt = chevron.render(check_agent_prompt_template, {"expectations": expectations})

        # Используйте dedent
        import textwrap
        user_prompt = textwrap.dedent(
        """
        Теперь, исходя из следующих характеристик собеседника, и соблюдая правила, данные ранее, 
        создайте свои вопросы и проведите с собеседником собеседование. Удачи!

        """)

        if include_agent_spec:
            user_prompt += f"\n\n{json.dumps(person._persona, indent=4)}"
        else:
            user_prompt += f"\n\nМини-биография собеседника: {person.minibio()}"


        logger = logging.getLogger("tinytroupe")

        logger.info(f"Начинается проверка персоны: {person.name}")

        # Отправка начальных сообщений в LLM
        current_messages.append({"role": "system", "content": system_prompt})
        current_messages.append({"role": "user", "content": user_prompt})

        message = openai_utils.client().send_message(current_messages)

        # Какая строка должна быть найдена для завершения разговора
        termination_mark = "```json"

        while message is not None and not (termination_mark in message["content"]):
            # Добавление вопросов к текущим сообщениям
            questions = message["content"]
            current_messages.append({"role": message["role"], "content": questions})
            logger.info(f"Проверка вопросов:\n{questions}")

            # Задавание вопросов собеседнику
            person.listen_and_act(questions, max_content_length=max_content_length)
            responses = person.pop_actions_and_get_contents_for("TALK", False)
            logger.info(f"Ответ персоны:\n{responses}")

            # Добавление ответов к текущему разговору и проверка следующего сообщения
            current_messages.append({"role": "user", "content": responses})
            message = openai_utils.client().send_message(current_messages)

        if message is not None:
            json_content = utils.extract_json(message['content'])
            # чтение оценки и обоснования
            score = float(json_content["score"])
            justification = json_content["justification"]
            logger.info(f"Оценка проверки: {score:.2f}; Обоснование: {justification}")

            return score, justification

        else:
            return None, None