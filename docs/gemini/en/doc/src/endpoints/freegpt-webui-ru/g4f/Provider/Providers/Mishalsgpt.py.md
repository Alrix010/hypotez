# Mishalsgpt Provider

## Overview

This module provides a `Mishalsgpt` provider for the `g4f` service. It handles communication with the MishalGPT API for generating text completions.

## Details

The `Mishalsgpt` provider is designed to work with the MishalGPT API. It uses the `requests` library for making HTTP requests to the API endpoint and handles the response data to provide a seamless integration with the `g4f` framework.

The provider supports streaming responses, enabling it to display output gradually as it is being generated by the AI model. It also offers options to customize the generation process, such as setting the temperature parameter for controlling the creativity of the generated text.

## Classes

### `_create_completion` 

**Description**: This function is used to generate a completion from the MishalGPT API. 

**Parameters**:

- `model (str)`: The name of the GPT model to use for generating the completion.
- `messages (list)`: A list of messages that will be used as context for the completion generation.
- `stream (bool)`:  Determines whether the response should be streamed.

**Returns**:

- `Generator[str, None, None]`: A generator that yields the generated text, providing streaming capability.

**Raises Exceptions**:

- `Exception`: If there is an error during the request or response handling.


**How the Function Works**:
-  The function constructs a JSON payload containing the model, temperature, and messages.
-  It then sends a POST request to the MishalGPT API endpoint using the `requests` library.
-  The response is parsed, and each completion is yielded as a string, enabling the stream functionality.


**Example**:

```python
# Assuming a list of messages is available
messages = [
    {'role': 'user', 'content': 'Hello, how are you?'},
]

# Create a completion with the gpt-3.5-turbo model, streaming enabled
completions = _create_completion(model='gpt-3.5-turbo', messages=messages, stream=True)

# Iterate through the generated text
for completion in completions:
    print(completion)
```

## Parameter Details

- `model (str)`: This parameter specifies the GPT model to be used for generating the completion. It can be either `gpt-3.5-turbo-16k-0613` or `gpt-3.5-turbo`.
- `messages (list)`: This parameter takes a list of dictionaries containing the messages to be used as context for the completion generation. Each message dictionary should have a `role` key specifying the role of the message (e.g., `user`, `system`) and a `content` key for the message text.
- `stream (bool)`: This parameter determines whether the response should be streamed. If set to `True`, the completion will be yielded as a generator, allowing for gradual output display.

## Examples

```python
# Example 1: Generating a completion with the gpt-3.5-turbo model
messages = [
    {'role': 'user', 'content': 'What is the capital of France?'},
]
completions = _create_completion(model='gpt-3.5-turbo', messages=messages, stream=True)
for completion in completions:
    print(completion)

# Example 2: Generating a completion with the gpt-3.5-turbo-16k-0613 model, streaming enabled
messages = [
    {'role': 'user', 'content': 'Write a short story about a cat who loves to eat pizza.'},
]
completions = _create_completion(model='gpt-3.5-turbo-16k-0613', messages=messages, stream=True)
for completion in completions:
    print(completion)
```