# Документация для `provider.py`

## Обзор

Файл `provider.py` предназначен для работы с локальными моделями GPT4All в проекте `hypotez`. Он содержит функции для поиска, загрузки и создания завершений с использованием локальных моделей.

## Подробнее

Этот модуль позволяет использовать локально установленные модели GPT4All для генерации текста. Он включает функции для поиска директории с моделями, загрузки моделей и создания завершений на основе предоставленных сообщений. Это позволяет избежать необходимости в онлайн-сервисах для выполнения задач генерации текста, что может быть полезно в условиях ограниченного доступа к интернету или для обеспечения конфиденциальности данных.

## Функции

### `find_model_dir`

```python
def find_model_dir(model_file: str) -> str:
    """
    Функция выполняет поиск директории, содержащей файл модели.

    Args:
        model_file (str): Имя файла модели.

    Returns:
        str: Путь к директории, содержащей файл модели.
             Если файл не найден, возвращает путь к новой директории для моделей.

    Как работает функция:
    - Функция пытается найти директорию, содержащую указанный файл модели, в нескольких местах:
      - В локальной директории модуля.
      - В директории `models` на уровень выше.
      - В текущей рабочей директории, обходя все поддиректории.
    - Если файл модели найден, функция возвращает путь к этой директории.
    - Если файл не найден ни в одном из указанных мест, функция возвращает путь к новой директории для моделей.
    """
```

### Классы

### `LocalProvider`

**Описание**:
Класс `LocalProvider` предоставляет статический метод для создания завершений с использованием локальных моделей GPT4All.

**Атрибуты**:
- Отсутствуют атрибуты класса.

**Методы**:
- `create_completion`: Статический метод для создания завершений.

### Методы класса

### `create_completion`

```python
    @staticmethod
    def create_completion(model: str, messages: Messages, stream: bool = False, **kwargs):
        """
        Создает завершение с использованием локальной модели GPT4All.

        Args:
            model (str): Имя модели.
            messages (Messages): Список сообщений для создания контекста завершения.
            stream (bool, optional): Определяет, должен ли вывод быть потоковым. По умолчанию `False`.
            **kwargs: Дополнительные аргументы.

        Returns:
            Generator[str, None, None] | str: Генератор токенов, если `stream` равен `True`, иначе строка с завершением.

        Raises:
            ValueError: Если модель не найдена или не реализована.
            ValueError: Если файл модели не найден.

         Как работает функция:
         - Проверяет, инициализирован ли список моделей (`MODEL_LIST`). Если нет, то вызывает функцию `get_models()` для его инициализации.
         - Проверяет, есть ли указанная модель в списке доступных моделей. Если нет, вызывает исключение `ValueError`.
         - Извлекает путь к файлу модели из списка моделей.
         - Определяет директорию, в которой находится файл модели, с помощью функции `find_model_dir()`.
         - Если файл модели не найден в указанной директории, предлагает пользователю загрузить его.
         - Инициализирует модель GPT4All с указанными параметрами.
         - Формирует системное сообщение и контекст разговора на основе предоставленных сообщений.
         - Создает сессию чата с моделью и генерирует завершение.
         - Если `stream` равен `True`, возвращает генератор токенов. В противном случае возвращает строку с завершением.

        Пример:
            >>> messages = [{"role": "user", "content": "Hello, how are you?"}]
            >>> model = "ggml-model-gpt4all-falcon-q4_0.bin"
            >>> completion = LocalProvider.create_completion(model, messages)
        """
```

### Внутренние функции

Внутри метода `create_completion` определена функция `should_not_stop`.

#### `should_not_stop`

```python
        def should_not_stop(token_id: int, token: str):
            """
            Определяет, следует ли остановить генерацию токенов.

            Args:
                token_id (int): Идентификатор текущего токена.
                token (str): Текущий токен.

            Returns:
                bool: `True`, если генерацию следует продолжить, `False` в противном случае.

           Как работает функция:
           - Функция проверяет, содержит ли текущий токен слово "USER".
           - Если токен содержит "USER", функция возвращает `False`, указывая на необходимость остановить генерацию.
           - В противном случае функция возвращает `True`, указывая на необходимость продолжить генерацию.
           - Это позволяет остановить генерацию, когда модель возвращает запрос пользователя.
        """
```

### Параметры класса

Нет параметров класса.

### Примеры

#### Пример использования `create_completion`

```python
messages = [{"role": "user", "content": "Hello, how are you?"}]
model = "ggml-model-gpt4all-falcon-q4_0.bin"
completion = LocalProvider.create_completion(model, messages)