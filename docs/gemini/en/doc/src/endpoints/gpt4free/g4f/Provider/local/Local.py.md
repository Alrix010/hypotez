# Документация модуля `Local.py`

## Обзор

Модуль `Local.py` предоставляет класс `Local`, который является адаптером для локальных моделей, таких как GPT4All. Он позволяет использовать эти модели через интерфейс, совместимый с другими провайдерами в проекте `hypotez`. Модуль поддерживает сохранение истории сообщений, системные сообщения и потоковую передачу данных.

## Более детально

Модуль предназначен для интеграции локальных моделей в систему `hypotez`, позволяя использовать их наравне с удаленными API. Это полезно для случаев, когда требуется конфиденциальность, отсутствует подключение к интернету или необходимо снизить задержки. Модуль проверяет наличие необходимых зависимостей (`gpt4all`) и предоставляет метод для создания завершений на основе выбранной модели и входных сообщений.

## Классы

### `Local`

**Описание**: Класс `Local` является адаптером для локальных моделей, такими как GPT4All.

**Наследует**:
- `AbstractProvider`: Абстрактный базовый класс для всех провайдеров.
- `ProviderModelMixin`: Миксин для работы с моделями провайдера.

**Атрибуты**:
- `label` (str): Метка провайдера, в данном случае `"GPT4All"`.
- `working` (bool): Флаг, указывающий, что провайдер работает, в данном случае `True`.
- `supports_message_history` (bool): Флаг, указывающий, что провайдер поддерживает историю сообщений, в данном случае `True`.
- `supports_system_message` (bool): Флаг, указывающий, что провайдер поддерживает системные сообщения, в данном случае `True`.
- `supports_stream` (bool): Флаг, указывающий, что провайдер поддерживает потоковую передачу, в данном случае `True`.
- `models` (list): Список доступных моделей.
- `default_model` (str): Модель по умолчанию.

**Принцип работы**:
1. При инициализации класса проверяется наличие необходимых зависимостей (`gpt4all`). Если они отсутствуют, выбрасывается исключение `MissingRequirementsError`.
2. Метод `create_completion` создает запрос на завершение текста, используя локальную модель.
3. Метод `get_models` возвращает список доступных моделей.

**Методы**:
- `get_models(cls)`: Возвращает список доступных моделей.
- `create_completion(cls, model: str, messages: Messages, stream: bool, **kwargs) -> CreateResult`: Создает завершение текста на основе выбранной модели и входных сообщений.

## Методы класса

### `get_models`

```python
    @classmethod
    def get_models(cls):
        """
        Получает список доступных моделей.

        Args:
            cls: Ссылка на класс `Local`.

        Returns:
            list: Список доступных моделей.
        """
```

### `create_completion`

```python
    @classmethod
    def create_completion(
        cls,
        model: str,
        messages: Messages,
        stream: bool,
        **kwargs
    ) -> CreateResult:
        """
        Создает завершение текста на основе выбранной модели и входных сообщений.

        Args:
            cls: Ссылка на класс `Local`.
            model (str): Название модели.
            messages (Messages): Список сообщений для модели.
            stream (bool): Флаг, указывающий, нужно ли использовать потоковую передачу.
            **kwargs: Дополнительные аргументы.

        Returns:
            CreateResult: Результат создания завершения.

        Raises:
            MissingRequirementsError: Если отсутствуют необходимые зависимости (`gpt4all`).

        """
```

## Параметры класса

- `label` (str): Метка провайдера, в данном случае `"GPT4All"`.
- `working` (bool): Флаг, указывающий, что провайдер работает, в данном случае `True`.
- `supports_message_history` (bool): Флаг, указывающий, что провайдер поддерживает историю сообщений, в данном случае `True`.
- `supports_system_message` (bool): Флаг, указывающий, что провайдер поддерживает системные сообщения, в данном случае `True`.
- `supports_stream` (bool): Флаг, указывающий, что провайдер поддерживает потоковую передачу, в данном случае `True`.
- `models` (list): Список доступных моделей.
- `default_model` (str): Модель по умолчанию.

**Примеры**

```python
# Пример получения списка моделей
models = Local.get_models()
print(models)

# Пример создания завершения
messages = [{"role": "user", "content": "Hello, how are you?"}]
try:
    result = Local.create_completion(model="default", messages=messages, stream=False)
    print(result)
except MissingRequirementsError as ex:
    print(f"Error: {ex}")