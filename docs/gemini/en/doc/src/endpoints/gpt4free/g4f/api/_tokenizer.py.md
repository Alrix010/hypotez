# Модуль _tokenizer

## Обзор

Модуль _tokenizer предоставляет функции для токенизации текста с использованием модели GPT-3.5-turbo. 

## Детали

Этот модуль предназначен для подсчета количества токенов в тексте и кодирования текста в токены, используя модель GPT-3.5-turbo. Он использует библиотеку `tiktoken` для этой задачи.

## Функции

### `tokenize`

**Цель**: Функция предназначена для токенизации текста, используя модель GPT-3.5-turbo, и возвращает количество токенов и закодированный текст.

**Параметры**:

- `text` (str): Текст для токенизации.
- `model` (str, optional): Модель для использования. По умолчанию `gpt-3.5-turbo`.

**Возвращает**:

- `tuple[int, list[int]]`: Кортеж, содержащий количество токенов и закодированный текст.

**Поднимает исключения**:

- `Exception`: Если возникает ошибка во время токенизации.

**Как работает функция**:

- Функция использует `tiktoken` для токенизации текста, используя заданную модель.
- Она подсчитывает количество токенов в закодированном тексте и возвращает его вместе с закодированным текстом.

**Примеры**:

```python
>>> tokenize("Hello, world!", model="gpt-3.5-turbo")
(3, [15496, 997, 628])

>>> tokenize("This is a longer text with more tokens.", model="gpt-3.5-turbo")
(8, [12490, 2057, 12490, 12490, 12490, 12490, 12490, 2057])
```

**Внутренние функции**:

- Функция не содержит внутренних функций.