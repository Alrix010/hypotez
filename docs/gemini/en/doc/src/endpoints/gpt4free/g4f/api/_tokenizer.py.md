# Документация модуля `_tokenizer.py`

## Обзор

Модуль предоставляет функции для токенизации текста с использованием библиотеки `tiktoken`. В настоящее время код закомментирован и не выполняет никаких действий.

## Более подробная информация

В данном файле предполагается реализация функции токенизации текста, которая могла бы использоваться для оценки количества токенов в тексте перед отправкой запроса к моделям, таким как GPT-3.5-turbo.

## Функции

### `tokenize`

```python
# def tokenize(text: str, model: str = 'gpt-3.5-turbo') -> Union[int, str]:
#     encoding   = tiktoken.encoding_for_model(model)
#     encoded    = encoding.encode(text)
#     num_tokens = len(encoded)
    
#     return num_tokens, encoded
```

**Назначение**: Функция для токенизации текста и подсчета количества токенов.

**Параметры**:
- `text` (str): Текст для токенизации.
- `model` (str): Название модели для токенизации (по умолчанию 'gpt-3.5-turbo').

**Возвращает**:
- `Union[int, str]`: Количество токенов в тексте.

**Как работает функция**:
1. Вычисляет кодировку для заданной модели (`tiktoken.encoding_for_model(model)`).
2. Кодирует текст с использованием полученной кодировки (`encoding.encode(text)`).
3. Определяет количество токенов в закодированном тексте (`len(encoded)`).
4. Возвращает количество токенов.

**Примеры**:

```python
# Пример использования (в закомментированном виде)
# num_tokens, encoded_text = tokenize("Пример текста для токенизации.", model='gpt-3.5-turbo')
# print(f"Количество токенов: {num_tokens}")
```